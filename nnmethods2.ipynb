{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnmethods2",
      "provenance": [],
      "authorship_tag": "ABX9TyOeYT8p87Yn+jr2tv5erG1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semkud/nnmethods/blob/main/nnmethods2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlI-BA5Hs2GW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5dfcd1f-d766-405b-f5cf-9ae19c6e1415"
      },
      "source": [
        "!pip install torchmetrics\n",
        "!pip install ipdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.0\n",
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.30.1-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.23-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 25.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=e53f559620a99d723560452b96d9636cec54b1d69ba57fb7162496da8721e946\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.23 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.30.1 prompt-toolkit-3.0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xXKOXBgvYJB"
      },
      "source": [
        "Загружаем данные, делим на обучающую и валидационную выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpxZdkrvIo4"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0\n",
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "#print(len(all_tweets_data))\n",
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]\n",
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhsaAqNa4mWG"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY3L96YE5Zxc",
        "outputId": "69c423b4-3a0e-4445-e052-53fd91fd5a45"
      },
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(preprocess(text))\n",
        "print('всего уникальных токенов:', len(vocab))\n",
        "\n",
        "\n",
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, вcтретившихся больше 2 раз:', len(filtered_vocab))\n",
        "#создаем словарь с индексами word2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2word = {i:word for word, i in word2id.items()}\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 202718\n",
            "уникальных токенов, вcтретившихся больше 2 раз: 32511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwf0Q58gCkFb"
      },
      "source": [
        "Обучаем эмбеддинговую модель на текстах наших твиттов, устанавливаем размер эмбеддинга - 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XOmJoq74pRA"
      },
      "source": [
        "import gensim\n",
        "texts = all_tweets_data.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1)\n",
        "\n",
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suQh6xqa_i50",
        "outputId": "b2cfa79b-402f-45ae-8939-760cc11c3f08"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32512, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkcb2HF26Wpr",
        "outputId": "b093a9e6-277e-49fa-f9d6-fd2e6ebf4842"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHYiy4qLFIKR"
      },
      "source": [
        "## Задача 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKjr_LzdCyVB"
      },
      "source": [
        "Класс Датасет для первой задачи нам подойдет с той пары, на которой мы рассматривали перептрон"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1dY2Hmp6Yx_"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.preprocess(self.dataset[index]) # токенизируем\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_ids, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qasdDFCx6kJ1"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jubhvYpHJlk"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AszoJV4CDJic"
      },
      "source": [
        "Пишем класс сетки.\n",
        "\n",
        "1.Первое это эмбеддинги - подгружаем заранее обученные, в параметре freeze ставим False, соответственно эмбеддинги тоже будут дообучаться.\n",
        "2.В сверточном слое у нас три фильтра с разными окнами: биграммы, биграммы со страйдом 2, триграмммы. У всех in_channels соответствует размеру эмбеддинга, out_channels - одинаковые, чтобы можно было конкатенировать по глубине.\n",
        "3.Еще один сверточный слой - 3 входных канала (это конкатеннация предыдущих трех фильтров), 50 выходных.\n",
        "4.Пуллинг везде - макс пуллинг овер тайм\n",
        "5.Наконец последний линейный слой на входе у него 50, соответствует выходу с последнего сверточного, на выходе 1\n",
        "6.Функция активации - сигмоида\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_5-biS5FeCY"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size):\n",
        "        \n",
        "        super().__init__()          \n",
        "        # указываем в атрибутах класса, какие слои и активации нам понадобятся\n",
        "        self.embedding = nn.Embedding(vocab_size, 100)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=False) #@@@\n",
        "        self.bigrams1 = nn.Conv1d(in_channels=100, out_channels=80, kernel_size=2, stride=1)\n",
        "        self.bigrams2 = nn.Conv1d(in_channels=100, out_channels=80, kernel_size=2, stride=2)\n",
        "        self.trigrams = nn.Conv1d(in_channels=100, out_channels=80, kernel_size=3, stride=1)\n",
        "        self.cnn2 = nn.Conv1d(in_channels=3, out_channels=50, kernel_size=3, stride=1) #new\n",
        "\n",
        "\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=50, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.out = nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "    def forward(self, text): #необходимый метод,  в нем указываем, как именно связываются слои/активации между собой\n",
        "        # batch_size x seq_len\n",
        "        #ipdb.set_trace()\n",
        "        embedded = self.embedding(text)   # переводим последовательность индексов в последовательность эмбеддингов\n",
        "        # batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams1 = self.dropout(self.pooling(self.relu(self.bigrams1(embedded))))\n",
        "        #batch_size x filter_count21 x seq_len*\n",
        "        feature_map_bigrams2 = self.dropout(self.pooling(self.relu(self.bigrams2(embedded))))\n",
        "        #batch_size x filter_count22 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling21 = feature_map_bigrams1.max(2)[0] \n",
        "        # batch_size x filter_count21\n",
        "        pooling22 = feature_map_bigrams2.max(2)[0] \n",
        "        # batch_size x filter_count22\n",
        "        pooling3 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "\n",
        "        concat = torch.stack((pooling21,pooling22,pooling3))\n",
        "        # 3 x batch_size x 80\n",
        "        concat = concat.transpose(0,1)\n",
        "        # batch_size x 3 x 80\n",
        "\n",
        "        feature_map_cnn2 = self.dropout(self.pooling(self.relu(self.cnn2(concat))))\n",
        "        poolinglast = feature_map_cnn2.max(2)[0]\n",
        "\n",
        "        logits = self.hidden(poolinglast)\n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cNk-_85HAI8"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке\n",
        "  \n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti1qZSH8E4YG",
        "outputId": "994baddf-0b5f-475f-a34c-1d4faae3ac06"
      },
      "source": [
        "model = MLP(len(word2id))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7658290714025497\n",
            "Train loss: 0.7268395297455065\n",
            "Train loss: 0.7123079967498779\n",
            "Train loss: 0.7033443566578538\n",
            "Train loss: 0.6963426378511247\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7037113197147846, Val f1: 0.5911188125610352\n",
            "Val loss: 0.6828516270175125, Val f1: 0.571102499961853\n",
            "Val loss: 0.6762748754024506, Val f1: 0.565504789352417\n",
            "Val loss: 0.6724078085885119, Val f1: 0.5647856593132019\n",
            "Val loss: 0.6701749868336178, Val f1: 0.5631181001663208\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3279680013656616, Val f1: 1.0777702331542969\n",
            "Val loss: 0.8848335941632589, Val f1: 0.7196509838104248\n",
            "Val loss: 0.7957148790359497, Val f1: 0.6581218838691711\n",
            "Val loss: 0.7579631209373474, Val f1: 0.626775860786438\n",
            "Val loss: 0.7369367943869697, Val f1: 0.6070888638496399\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6998934969305992\n",
            "Train loss: 0.6750970446702206\n",
            "Train loss: 0.6649915993213653\n",
            "Train loss: 0.6587339452843168\n",
            "Train loss: 0.6539941508145559\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6685661487281322, Val f1: 0.6601027250289917\n",
            "Val loss: 0.6461264454957211, Val f1: 0.6437341570854187\n",
            "Val loss: 0.6394485485553741, Val f1: 0.6375881433486938\n",
            "Val loss: 0.6368110037561673, Val f1: 0.634594738483429\n",
            "Val loss: 0.6348369107359931, Val f1: 0.6344391107559204\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2716933488845825, Val f1: 1.185625672340393\n",
            "Val loss: 0.8469050725301107, Val f1: 0.798414409160614\n",
            "Val loss: 0.7616386413574219, Val f1: 0.7202928066253662\n",
            "Val loss: 0.7248001183782306, Val f1: 0.6888344287872314\n",
            "Val loss: 0.7041639420721266, Val f1: 0.6702666878700256\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6613192297518253\n",
            "Train loss: 0.6405589995962201\n",
            "Train loss: 0.632856707572937\n",
            "Train loss: 0.6285620225009634\n",
            "Train loss: 0.6252608781769162\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6375500671565533, Val f1: 0.7020473480224609\n",
            "Val loss: 0.6180480122566223, Val f1: 0.6800381541252136\n",
            "Val loss: 0.6119226598739624, Val f1: 0.6755222082138062\n",
            "Val loss: 0.6085096099483434, Val f1: 0.6724398136138916\n",
            "Val loss: 0.6069404035806656, Val f1: 0.6708423495292664\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2325929999351501, Val f1: 1.257580280303955\n",
            "Val loss: 0.8236313462257385, Val f1: 0.8268331289291382\n",
            "Val loss: 0.7406777977943421, Val f1: 0.7516859173774719\n",
            "Val loss: 0.7040126664297921, Val f1: 0.7197068929672241\n",
            "Val loss: 0.6838207112418281, Val f1: 0.7001780271530151\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6395922526717186\n",
            "Train loss: 0.6182928898117759\n",
            "Train loss: 0.608983360528946\n",
            "Train loss: 0.603930415502235\n",
            "Train loss: 0.6007338471355892\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6116667427122593, Val f1: 0.7229642271995544\n",
            "Val loss: 0.5938640254916567, Val f1: 0.7010049223899841\n",
            "Val loss: 0.5890351486206055, Val f1: 0.691635012626648\n",
            "Val loss: 0.5851370994724444, Val f1: 0.6892566084861755\n",
            "Val loss: 0.5827244754348483, Val f1: 0.6883963942527771\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1978939175605774, Val f1: 1.29290771484375\n",
            "Val loss: 0.8042137424151102, Val f1: 0.849922776222229\n",
            "Val loss: 0.7242024302482605, Val f1: 0.7664517760276794\n",
            "Val loss: 0.688624484198434, Val f1: 0.7319172620773315\n",
            "Val loss: 0.6688910457823012, Val f1: 0.7120806574821472\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6093205101788044\n",
            "Train loss: 0.589817534793507\n",
            "Train loss: 0.5844742143154145\n",
            "Train loss: 0.5801944768250878\n",
            "Train loss: 0.5773224064282009\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5927771665155888, Val f1: 0.7321383953094482\n",
            "Val loss: 0.5716284856651769, Val f1: 0.7146448493003845\n",
            "Val loss: 0.5654228401184082, Val f1: 0.7088100910186768\n",
            "Val loss: 0.5616255758413627, Val f1: 0.7062065601348877\n",
            "Val loss: 0.5595015415123531, Val f1: 0.70469731092453\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1726343631744385, Val f1: 1.312835931777954\n",
            "Val loss: 0.7914124925931295, Val f1: 0.8518385887145996\n",
            "Val loss: 0.7125561714172364, Val f1: 0.764999270439148\n",
            "Val loss: 0.6761842966079712, Val f1: 0.7335488796234131\n",
            "Val loss: 0.6572753588358561, Val f1: 0.7128908038139343\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5888975337147713\n",
            "Train loss: 0.5685797785267686\n",
            "Train loss: 0.5620831310749054\n",
            "Train loss: 0.5586996638952796\n",
            "Train loss: 0.5564787458805811\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5597167648375034, Val f1: 0.7766628861427307\n",
            "Val loss: 0.544184477040262, Val f1: 0.7498719096183777\n",
            "Val loss: 0.5397485995292663, Val f1: 0.7409237623214722\n",
            "Val loss: 0.5376344928100928, Val f1: 0.73794025182724\n",
            "Val loss: 0.536234802078633, Val f1: 0.7355318665504456\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1490790843963623, Val f1: 1.3536393642425537\n",
            "Val loss: 0.7775741616884867, Val f1: 0.8786813020706177\n",
            "Val loss: 0.6990685820579529, Val f1: 0.7934088110923767\n",
            "Val loss: 0.6634770376341683, Val f1: 0.7611936926841736\n",
            "Val loss: 0.644715752866533, Val f1: 0.7389029860496521\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5670052990317345\n",
            "Train loss: 0.546614919648026\n",
            "Train loss: 0.5398131811618805\n",
            "Train loss: 0.5361960881681584\n",
            "Train loss: 0.53432342871314\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5354940313845873, Val f1: 0.7965494394302368\n",
            "Val loss: 0.5227580937472257, Val f1: 0.7695710062980652\n",
            "Val loss: 0.5175766617059707, Val f1: 0.7635108232498169\n",
            "Val loss: 0.5148685449984536, Val f1: 0.7605355381965637\n",
            "Val loss: 0.5128949823833647, Val f1: 0.7590729594230652\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1270884275436401, Val f1: 1.37458336353302\n",
            "Val loss: 0.763905942440033, Val f1: 0.8956004977226257\n",
            "Val loss: 0.6872810482978821, Val f1: 0.8081237077713013\n",
            "Val loss: 0.6521132418087551, Val f1: 0.7758676409721375\n",
            "Val loss: 0.6341699759165446, Val f1: 0.7525027990341187\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5411356426775455\n",
            "Train loss: 0.5239797938953746\n",
            "Train loss: 0.5186485970020294\n",
            "Train loss: 0.516015126634\n",
            "Train loss: 0.5133190158577192\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5129418782889843, Val f1: 0.8300533294677734\n",
            "Val loss: 0.4994199872016907, Val f1: 0.8047131299972534\n",
            "Val loss: 0.4956619656085968, Val f1: 0.7954985499382019\n",
            "Val loss: 0.4935245820835455, Val f1: 0.7913078665733337\n",
            "Val loss: 0.4920755472211611, Val f1: 0.7888211607933044\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1183778047561646, Val f1: 1.4128111600875854\n",
            "Val loss: 0.7586556673049927, Val f1: 0.9348620176315308\n",
            "Val loss: 0.6812190890312195, Val f1: 0.8433955311775208\n",
            "Val loss: 0.6443373901503426, Val f1: 0.812221884727478\n",
            "Val loss: 0.6264949639638265, Val f1: 0.7884831428527832\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5230912640690804\n",
            "Train loss: 0.5053834788727037\n",
            "Train loss: 0.4987349587678909\n",
            "Train loss: 0.495651680586943\n",
            "Train loss: 0.4938492654334931\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4975613206624985, Val f1: 0.8304229974746704\n",
            "Val loss: 0.48103744875301013, Val f1: 0.8067005276679993\n",
            "Val loss: 0.47466528236865996, Val f1: 0.8012546896934509\n",
            "Val loss: 0.4716199018172364, Val f1: 0.7976202964782715\n",
            "Val loss: 0.470254714290301, Val f1: 0.7963306307792664\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0965773463249207, Val f1: 1.4063212871551514\n",
            "Val loss: 0.7459988196690878, Val f1: 0.9215357303619385\n",
            "Val loss: 0.6705374002456665, Val f1: 0.8327566385269165\n",
            "Val loss: 0.6354022877556937, Val f1: 0.8011291027069092\n",
            "Val loss: 0.6183936397234598, Val f1: 0.7747780084609985\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.4956248700618744\n",
            "Train loss: 0.48301991368785047\n",
            "Train loss: 0.4775960803031921\n",
            "Train loss: 0.47330968726926775\n",
            "Train loss: 0.4724136253907567\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4687796216458082, Val f1: 0.8544764518737793\n",
            "Val loss: 0.4546208146846656, Val f1: 0.8291308283805847\n",
            "Val loss: 0.45061285674571994, Val f1: 0.8184608817100525\n",
            "Val loss: 0.4493730730974852, Val f1: 0.8140230774879456\n",
            "Val loss: 0.4472519304780733, Val f1: 0.8110366463661194\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0892894268035889, Val f1: 1.4165375232696533\n",
            "Val loss: 0.7407923539479574, Val f1: 0.9282974600791931\n",
            "Val loss: 0.6656136989593506, Val f1: 0.8385006189346313\n",
            "Val loss: 0.6309966274670192, Val f1: 0.8058257699012756\n",
            "Val loss: 0.6138675543997023, Val f1: 0.7803514003753662\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.47886661626398563\n",
            "Train loss: 0.4640975567427548\n",
            "Train loss: 0.45810810089111326\n",
            "Train loss: 0.4564392628954418\n",
            "Train loss: 0.4552043740238462\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4483447764068842, Val f1: 0.8677850365638733\n",
            "Val loss: 0.4335187547134631, Val f1: 0.8406012058258057\n",
            "Val loss: 0.4292599201202393, Val f1: 0.8332840800285339\n",
            "Val loss: 0.4273698850354152, Val f1: 0.8304628133773804\n",
            "Val loss: 0.42656530085064115, Val f1: 0.8273063898086548\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0851081609725952, Val f1: 1.4309053421020508\n",
            "Val loss: 0.7384910782178243, Val f1: 0.9375877380371094\n",
            "Val loss: 0.6638574004173279, Val f1: 0.8462986350059509\n",
            "Val loss: 0.628144383430481, Val f1: 0.8135406970977783\n",
            "Val loss: 0.6113796565267775, Val f1: 0.7880118489265442\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.45377749018371105\n",
            "Train loss: 0.4426545563972358\n",
            "Train loss: 0.43830212235450744\n",
            "Train loss: 0.4375106367602277\n",
            "Train loss: 0.4357915449710119\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42651091143488884, Val f1: 0.8837716579437256\n",
            "Val loss: 0.41342579234730115, Val f1: 0.8538684844970703\n",
            "Val loss: 0.40921590924263, Val f1: 0.8448548913002014\n",
            "Val loss: 0.4070371367148499, Val f1: 0.8402015566825867\n",
            "Val loss: 0.40587359560387476, Val f1: 0.8373636603355408\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0788277387619019, Val f1: 1.4242017269134521\n",
            "Val loss: 0.7366622090339661, Val f1: 0.935569703578949\n",
            "Val loss: 0.661594808101654, Val f1: 0.8469473123550415\n",
            "Val loss: 0.6261714271136692, Val f1: 0.8125384449958801\n",
            "Val loss: 0.6089012490378486, Val f1: 0.7877451181411743\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4339510668069124\n",
            "Train loss: 0.42070630192756653\n",
            "Train loss: 0.41786819338798525\n",
            "Train loss: 0.41653940348482843\n",
            "Train loss: 0.4171865437002409\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4054177552461624, Val f1: 0.8939650654792786\n",
            "Val loss: 0.3916068095149416, Val f1: 0.8651973009109497\n",
            "Val loss: 0.389402414560318, Val f1: 0.8554326295852661\n",
            "Val loss: 0.38749308506054664, Val f1: 0.85187828540802\n",
            "Val loss: 0.3862391483216059, Val f1: 0.8494988679885864\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0787947177886963, Val f1: 1.4226194620132446\n",
            "Val loss: 0.7349255681037903, Val f1: 0.9390646815299988\n",
            "Val loss: 0.6596328258514405, Val f1: 0.848054826259613\n",
            "Val loss: 0.6242660454341343, Val f1: 0.811954140663147\n",
            "Val loss: 0.6081935299767388, Val f1: 0.7859944105148315\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.4154322315007448\n",
            "Train loss: 0.4049134678912885\n",
            "Train loss: 0.4019773358106613\n",
            "Train loss: 0.39986857504987006\n",
            "Train loss: 0.40083252177351997\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3848695307970047, Val f1: 0.911104142665863\n",
            "Val loss: 0.3751439090931054, Val f1: 0.8830087780952454\n",
            "Val loss: 0.37092211544513704, Val f1: 0.8744763731956482\n",
            "Val loss: 0.36818403939702615, Val f1: 0.871276319026947\n",
            "Val loss: 0.36682401029836564, Val f1: 0.8689653277397156\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0840095281600952, Val f1: 1.4526972770690918\n",
            "Val loss: 0.7384854555130005, Val f1: 0.9574508666992188\n",
            "Val loss: 0.6629132390022278, Val f1: 0.865058422088623\n",
            "Val loss: 0.6280383127076286, Val f1: 0.8292362689971924\n",
            "Val loss: 0.6106147567431132, Val f1: 0.8020772337913513\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.40373681858181953\n",
            "Train loss: 0.38878392270117096\n",
            "Train loss: 0.38531277418136595\n",
            "Train loss: 0.38234273310917527\n",
            "Train loss: 0.3814820040549551\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36596741899847984, Val f1: 0.9131829738616943\n",
            "Val loss: 0.35420319167050446, Val f1: 0.8846279382705688\n",
            "Val loss: 0.34958936274051666, Val f1: 0.8758836388587952\n",
            "Val loss: 0.34720267866974447, Val f1: 0.8727015256881714\n",
            "Val loss: 0.3460992918837638, Val f1: 0.8703657984733582\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0860784649848938, Val f1: 1.4427998065948486\n",
            "Val loss: 0.7426721851030985, Val f1: 0.9456965923309326\n",
            "Val loss: 0.6666027903556824, Val f1: 0.8511902689933777\n",
            "Val loss: 0.6312369363648551, Val f1: 0.8163635730743408\n",
            "Val loss: 0.6142453220155504, Val f1: 0.7899454236030579\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.3842378742992878\n",
            "Train loss: 0.3705419997374217\n",
            "Train loss: 0.3641879069805145\n",
            "Train loss: 0.36405618244142673\n",
            "Train loss: 0.36339722715672995\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3463097158819437, Val f1: 0.9260757565498352\n",
            "Val loss: 0.33306552785815613, Val f1: 0.8999037742614746\n",
            "Val loss: 0.3293041801452637, Val f1: 0.8907088041305542\n",
            "Val loss: 0.3271434382716222, Val f1: 0.8871258497238159\n",
            "Val loss: 0.3258120907204492, Val f1: 0.8851791620254517\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0888869166374207, Val f1: 1.4602192640304565\n",
            "Val loss: 0.7446069121360779, Val f1: 0.9541960954666138\n",
            "Val loss: 0.6670410633087158, Val f1: 0.8592215776443481\n",
            "Val loss: 0.6316601463726589, Val f1: 0.8243662714958191\n",
            "Val loss: 0.614916099442376, Val f1: 0.7991840839385986\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.35614726319909096\n",
            "Train loss: 0.34934857126438257\n",
            "Train loss: 0.34853752970695495\n",
            "Train loss: 0.3484575263599851\n",
            "Train loss: 0.347969817618529\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3244535028934479, Val f1: 0.9396950602531433\n",
            "Val loss: 0.3158257423025189, Val f1: 0.9095368385314941\n",
            "Val loss: 0.3113296550512314, Val f1: 0.8998719453811646\n",
            "Val loss: 0.309793527446576, Val f1: 0.8952171206474304\n",
            "Val loss: 0.30803074616761433, Val f1: 0.8930675983428955\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0980748534202576, Val f1: 1.4612431526184082\n",
            "Val loss: 0.7497908671696981, Val f1: 0.9563645124435425\n",
            "Val loss: 0.6734517812728882, Val f1: 0.8570839166641235\n",
            "Val loss: 0.6381238528660366, Val f1: 0.8224433064460754\n",
            "Val loss: 0.6207718650499979, Val f1: 0.7972298860549927\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.34109559282660484\n",
            "Train loss: 0.3340563304496534\n",
            "Train loss: 0.33223811447620394\n",
            "Train loss: 0.3311283237008906\n",
            "Train loss: 0.3315738368602026\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3012102823704481, Val f1: 0.9527714252471924\n",
            "Val loss: 0.292580628936941, Val f1: 0.9229457974433899\n",
            "Val loss: 0.29067485392093656, Val f1: 0.9124354124069214\n",
            "Val loss: 0.2910638153553009, Val f1: 0.9065068960189819\n",
            "Val loss: 0.29011558386541547, Val f1: 0.9035220742225647\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.108995497226715, Val f1: 1.4565802812576294\n",
            "Val loss: 0.7575162847836813, Val f1: 0.9532107710838318\n",
            "Val loss: 0.6787718772888184, Val f1: 0.8558678030967712\n",
            "Val loss: 0.6431655287742615, Val f1: 0.8193265795707703\n",
            "Val loss: 0.625660757223765, Val f1: 0.7949734926223755\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.3290933296084404\n",
            "Train loss: 0.31757176915804547\n",
            "Train loss: 0.3170703011751175\n",
            "Train loss: 0.3165960325233972\n",
            "Train loss: 0.31522623059295474\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2815779345110059, Val f1: 0.9654984474182129\n",
            "Val loss: 0.27523700486529956, Val f1: 0.9334669709205627\n",
            "Val loss: 0.2723051854968071, Val f1: 0.9238755106925964\n",
            "Val loss: 0.27053947413145607, Val f1: 0.9197440147399902\n",
            "Val loss: 0.2705523602309681, Val f1: 0.9163467884063721\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1291861534118652, Val f1: 1.455540418624878\n",
            "Val loss: 0.7691902120908102, Val f1: 0.9606601595878601\n",
            "Val loss: 0.6874126076698304, Val f1: 0.8641935586929321\n",
            "Val loss: 0.6503720964704242, Val f1: 0.8295647501945496\n",
            "Val loss: 0.6326896084679497, Val f1: 0.8022564053535461\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.31146649084985256\n",
            "Train loss: 0.30223063627878827\n",
            "Train loss: 0.3009628123044968\n",
            "Train loss: 0.2998705581942601\n",
            "Train loss: 0.29960364422627855\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.26818588469177485, Val f1: 0.9719522595405579\n",
            "Val loss: 0.2575024741165566, Val f1: 0.9445267915725708\n",
            "Val loss: 0.25575775355100633, Val f1: 0.9349477887153625\n",
            "Val loss: 0.2551346986151453, Val f1: 0.9303681254386902\n",
            "Val loss: 0.25426453227798146, Val f1: 0.9275320172309875\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1361178755760193, Val f1: 1.460373044013977\n",
            "Val loss: 0.7735443909962972, Val f1: 0.9610103368759155\n",
            "Val loss: 0.6925742864608765, Val f1: 0.8644129633903503\n",
            "Val loss: 0.6557849049568176, Val f1: 0.8278390169143677\n",
            "Val loss: 0.6378246943155924, Val f1: 0.8017466068267822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zzuIuSy7HTlE",
        "outputId": "b07c2a5d-cba1-4fa7-9112-09856345a978"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "qPMIGMrVLbnx",
        "outputId": "1bfed845-cb00-4ec9-9727-7c6f2c6f983f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94uTUHS2FSyh"
      },
      "source": [
        "Как мы видим, модель обучается, и более того, довольно быстро переобучается, потому что функция потерь на обучении падает намного быстрее чем на тесте."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XcHqYMhHFW5"
      },
      "source": [
        "Попробуем немножко усовершенствовать модель. Увеличим лернинг рейт до 0.001, изменим dropout на 0.5, уменьшим число нейронов на линейном слое до 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHCUFzKHXR5"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size):\n",
        "        \n",
        "        super().__init__()          \n",
        "        # указываем в атрибутах класса, какие слои и активации нам понадобятся\n",
        "        self.embedding = nn.Embedding(vocab_size, 100)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=False) #@@@\n",
        "        self.bigrams1 = nn.Conv1d(in_channels=100, out_channels=80, kernel_size=2, stride=1)\n",
        "        self.bigrams2 = nn.Conv1d(in_channels=100, out_channels=80, kernel_size=2, stride=2)\n",
        "        self.trigrams = nn.Conv1d(in_channels=100, out_channels=80, kernel_size=3, stride=1)\n",
        "        self.cnn2 = nn.Conv1d(in_channels=3, out_channels=20, kernel_size=3, stride=1) #new\n",
        "\n",
        "\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=20, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "    def forward(self, text): #необходимый метод,  в нем указываем, как именно связываются слои/активации между собой\n",
        "        # batch_size x seq_len\n",
        "        #ipdb.set_trace()\n",
        "        embedded = self.embedding(text)   # переводим последовательность индексов в последовательность эмбеддингов\n",
        "        # batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams1 = self.dropout(self.pooling(self.relu(self.bigrams1(embedded))))\n",
        "        #batch_size x filter_count21 x seq_len*\n",
        "        feature_map_bigrams2 = self.dropout(self.pooling(self.relu(self.bigrams2(embedded))))\n",
        "        #batch_size x filter_count22 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling21 = feature_map_bigrams1.max(2)[0] \n",
        "        # batch_size x filter_count21\n",
        "        pooling22 = feature_map_bigrams2.max(2)[0] \n",
        "        # batch_size x filter_count22\n",
        "        pooling3 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "\n",
        "        concat = torch.stack((pooling21,pooling22,pooling3))\n",
        "        # 3 x batch_size x 80\n",
        "        concat = concat.transpose(0,1)\n",
        "        # batch_size x 3 x 80\n",
        "\n",
        "        feature_map_cnn2 = self.dropout(self.pooling(self.relu(self.cnn2(concat))))\n",
        "        poolinglast = feature_map_cnn2.max(2)[0]\n",
        "\n",
        "        logits = self.hidden(poolinglast)\n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mK2BsU-HkWz",
        "outputId": "01fc8158-15ba-476b-9623-0cd78260a16e"
      },
      "source": [
        "model = MLP(len(word2id))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7578085474669933\n",
            "Train loss: 0.724496765570207\n",
            "Train loss: 0.71253999710083\n",
            "Train loss: 0.705091159735153\n",
            "Train loss: 0.6989562887521017\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7130956277251244, Val f1: 0.36950939893722534\n",
            "Val loss: 0.6911896340774767, Val f1: 0.35730603337287903\n",
            "Val loss: 0.6843758583068847, Val f1: 0.35134172439575195\n",
            "Val loss: 0.6807116074348564, Val f1: 0.3496338725090027\n",
            "Val loss: 0.6788044316428048, Val f1: 0.3471020460128784\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3452885746955872, Val f1: 0.6145856380462646\n",
            "Val loss: 0.8948387702306112, Val f1: 0.43282896280288696\n",
            "Val loss: 0.8052633762359619, Val f1: 0.39302632212638855\n",
            "Val loss: 0.7672274964196342, Val f1: 0.37232863903045654\n",
            "Val loss: 0.7459951705402799, Val f1: 0.3620135188102722\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.7093257866799831\n",
            "Train loss: 0.6861619479728468\n",
            "Train loss: 0.6768670046329498\n",
            "Train loss: 0.6722302490205907\n",
            "Train loss: 0.6682226232119969\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6942392997443676, Val f1: 0.47859853506088257\n",
            "Val loss: 0.6736539241039392, Val f1: 0.4653283357620239\n",
            "Val loss: 0.6671089768409729, Val f1: 0.4579322636127472\n",
            "Val loss: 0.664373739441829, Val f1: 0.45548421144485474\n",
            "Val loss: 0.6622154982317061, Val f1: 0.45436424016952515\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3167001605033875, Val f1: 0.8215144872665405\n",
            "Val loss: 0.8760788043340048, Val f1: 0.5655989646911621\n",
            "Val loss: 0.7879833936691284, Val f1: 0.5181635022163391\n",
            "Val loss: 0.750620288508279, Val f1: 0.49349063634872437\n",
            "Val loss: 0.7295219302177429, Val f1: 0.4736039936542511\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6868027076125145\n",
            "Train loss: 0.6658045920458707\n",
            "Train loss: 0.6593080592155457\n",
            "Train loss: 0.6538362120514485\n",
            "Train loss: 0.6509031028974623\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6874943636357784, Val f1: 0.449349045753479\n",
            "Val loss: 0.6644994526198416, Val f1: 0.4440328776836395\n",
            "Val loss: 0.6579106616973877, Val f1: 0.43948450684547424\n",
            "Val loss: 0.6550285825088843, Val f1: 0.436218798160553\n",
            "Val loss: 0.6529088694424856, Val f1: 0.4369291663169861\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3004432916641235, Val f1: 0.8167992830276489\n",
            "Val loss: 0.8659112254778544, Val f1: 0.5407220125198364\n",
            "Val loss: 0.7790944814682007, Val f1: 0.4924996495246887\n",
            "Val loss: 0.7422360181808472, Val f1: 0.465387761592865\n",
            "Val loss: 0.7214745481808981, Val f1: 0.44976377487182617\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6690242551267147\n",
            "Train loss: 0.6474569909500353\n",
            "Train loss: 0.6404518604278564\n",
            "Train loss: 0.6361729991969778\n",
            "Train loss: 0.6321685903129124\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6713930740952492, Val f1: 0.6073808073997498\n",
            "Val loss: 0.6504256490505103, Val f1: 0.5893648266792297\n",
            "Val loss: 0.6430906867980957, Val f1: 0.585282027721405\n",
            "Val loss: 0.6400355100631714, Val f1: 0.5813896059989929\n",
            "Val loss: 0.6376893697750001, Val f1: 0.5806365013122559\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2767634987831116, Val f1: 1.053030252456665\n",
            "Val loss: 0.8511995474497477, Val f1: 0.701987624168396\n",
            "Val loss: 0.7651733040809632, Val f1: 0.6477617621421814\n",
            "Val loss: 0.7285092558179583, Val f1: 0.616799533367157\n",
            "Val loss: 0.708063628938463, Val f1: 0.6005500555038452\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6509355865418911\n",
            "Train loss: 0.6286174907828822\n",
            "Train loss: 0.6212064301967621\n",
            "Train loss: 0.6175247101641413\n",
            "Train loss: 0.6146394851661864\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.657164741307497, Val f1: 0.6221436262130737\n",
            "Val loss: 0.6375961556579127, Val f1: 0.5976632833480835\n",
            "Val loss: 0.631357593536377, Val f1: 0.591659426689148\n",
            "Val loss: 0.6279749905885156, Val f1: 0.5895177721977234\n",
            "Val loss: 0.626470345116797, Val f1: 0.5856971740722656\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2596638798713684, Val f1: 1.0742510557174683\n",
            "Val loss: 0.8401800394058228, Val f1: 0.7049867510795593\n",
            "Val loss: 0.7552647471427918, Val f1: 0.6429439783096313\n",
            "Val loss: 0.7193625739642552, Val f1: 0.6138566136360168\n",
            "Val loss: 0.698907302485572, Val f1: 0.5929188132286072\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6302565559744835\n",
            "Train loss: 0.6082177559534708\n",
            "Train loss: 0.6032129156589509\n",
            "Train loss: 0.5979154875029379\n",
            "Train loss: 0.5955332347324916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6388669833540916, Val f1: 0.6823824048042297\n",
            "Val loss: 0.6206360152273467, Val f1: 0.6564385890960693\n",
            "Val loss: 0.6154935145378113, Val f1: 0.6481440663337708\n",
            "Val loss: 0.6129939013452672, Val f1: 0.643510639667511\n",
            "Val loss: 0.6111695276839393, Val f1: 0.6422075033187866\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2381152510643005, Val f1: 1.177107572555542\n",
            "Val loss: 0.8267465631167094, Val f1: 0.7582955360412598\n",
            "Val loss: 0.742458975315094, Val f1: 0.6928676962852478\n",
            "Val loss: 0.7069490211350578, Val f1: 0.661048173904419\n",
            "Val loss: 0.6867688430680169, Val f1: 0.6429764628410339\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6051126271486282\n",
            "Train loss: 0.585198214559844\n",
            "Train loss: 0.5805646538734436\n",
            "Train loss: 0.5771352097169676\n",
            "Train loss: 0.5753739114318576\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6291579268872738, Val f1: 0.6870293617248535\n",
            "Val loss: 0.6102395129926277, Val f1: 0.6639778017997742\n",
            "Val loss: 0.6037115180492401, Val f1: 0.657070517539978\n",
            "Val loss: 0.600562919431658, Val f1: 0.6539093255996704\n",
            "Val loss: 0.5983549370652154, Val f1: 0.6524816155433655\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2199797630310059, Val f1: 1.1981301307678223\n",
            "Val loss: 0.8164096275965372, Val f1: 0.7709827423095703\n",
            "Val loss: 0.7330116271972656, Val f1: 0.7026302218437195\n",
            "Val loss: 0.6980447087969098, Val f1: 0.6657900214195251\n",
            "Val loss: 0.6784146030743917, Val f1: 0.6467719078063965\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5846595801413059\n",
            "Train loss: 0.5698139902317163\n",
            "Train loss: 0.5618367314338684\n",
            "Train loss: 0.5573639327020787\n",
            "Train loss: 0.554544278553554\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6118409745395184, Val f1: 0.7192559242248535\n",
            "Val loss: 0.5950920383135477, Val f1: 0.6964443922042847\n",
            "Val loss: 0.588506988286972, Val f1: 0.6885823011398315\n",
            "Val loss: 0.5855233366809675, Val f1: 0.6854467988014221\n",
            "Val loss: 0.5833202522425425, Val f1: 0.6844866275787354\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1972737312316895, Val f1: 1.232320785522461\n",
            "Val loss: 0.8018285830815634, Val f1: 0.7989152669906616\n",
            "Val loss: 0.7200409293174743, Val f1: 0.7286477088928223\n",
            "Val loss: 0.6863169755254473, Val f1: 0.6918507218360901\n",
            "Val loss: 0.666839513513777, Val f1: 0.6701323390007019\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5619150102138519\n",
            "Train loss: 0.5475439916957509\n",
            "Train loss: 0.5413290119171142\n",
            "Train loss: 0.5367707254281685\n",
            "Train loss: 0.5348544695547649\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.599597804248333, Val f1: 0.7364334464073181\n",
            "Val loss: 0.5800044319846414, Val f1: 0.7175195217132568\n",
            "Val loss: 0.5747026658058166, Val f1: 0.7095803022384644\n",
            "Val loss: 0.571730383296511, Val f1: 0.707137405872345\n",
            "Val loss: 0.5693683602980205, Val f1: 0.7057971358299255\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.178633987903595, Val f1: 1.2679228782653809\n",
            "Val loss: 0.7904926935831705, Val f1: 0.8227791786193848\n",
            "Val loss: 0.7100671529769897, Val f1: 0.7505189776420593\n",
            "Val loss: 0.6769010850361415, Val f1: 0.7122096419334412\n",
            "Val loss: 0.6574637624952528, Val f1: 0.6912289261817932\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5382899716496468\n",
            "Train loss: 0.5208959985863079\n",
            "Train loss: 0.5162479490041733\n",
            "Train loss: 0.5142348871302249\n",
            "Train loss: 0.5147015544630232\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5850993059575558, Val f1: 0.7906337380409241\n",
            "Val loss: 0.5644817424542976, Val f1: 0.7708066701889038\n",
            "Val loss: 0.558921959400177, Val f1: 0.7600978016853333\n",
            "Val loss: 0.5561352736914336, Val f1: 0.754833996295929\n",
            "Val loss: 0.5551732118640628, Val f1: 0.7514671683311462\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.162967324256897, Val f1: 1.322399616241455\n",
            "Val loss: 0.7801556785901388, Val f1: 0.8635752201080322\n",
            "Val loss: 0.7000976085662842, Val f1: 0.7816731333732605\n",
            "Val loss: 0.66728504214968, Val f1: 0.745568573474884\n",
            "Val loss: 0.6475396288765801, Val f1: 0.7271347045898438\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.5243440587073565\n",
            "Train loss: 0.5031748432101626\n",
            "Train loss: 0.4989279299974442\n",
            "Train loss: 0.4957098120184087\n",
            "Train loss: 0.49497444927692413\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5724992156028748, Val f1: 0.776305079460144\n",
            "Val loss: 0.552197768832698, Val f1: 0.7610086798667908\n",
            "Val loss: 0.5471822953224182, Val f1: 0.7524308562278748\n",
            "Val loss: 0.5447265799365827, Val f1: 0.7486333250999451\n",
            "Val loss: 0.5429349002384004, Val f1: 0.7462300062179565\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.149608075618744, Val f1: 1.2988828420639038\n",
            "Val loss: 0.7712519963582357, Val f1: 0.8450168967247009\n",
            "Val loss: 0.6921984791755676, Val f1: 0.7689626812934875\n",
            "Val loss: 0.660274156502315, Val f1: 0.7316128611564636\n",
            "Val loss: 0.6413493156433105, Val f1: 0.7124127745628357\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.5028723981231451\n",
            "Train loss: 0.4870974286036058\n",
            "Train loss: 0.48213721334934234\n",
            "Train loss: 0.48060732012364404\n",
            "Train loss: 0.4775302097910926\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5511046573519707, Val f1: 0.8176926970481873\n",
            "Val loss: 0.5343343976772192, Val f1: 0.794340968132019\n",
            "Val loss: 0.530504789352417, Val f1: 0.7850919961929321\n",
            "Val loss: 0.5278026185818573, Val f1: 0.7816653847694397\n",
            "Val loss: 0.5262601226568222, Val f1: 0.7801677584648132\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.129442572593689, Val f1: 1.3601104021072388\n",
            "Val loss: 0.7583157221476237, Val f1: 0.891032874584198\n",
            "Val loss: 0.680994701385498, Val f1: 0.8083502054214478\n",
            "Val loss: 0.649853093283517, Val f1: 0.7681725025177002\n",
            "Val loss: 0.6310049891471863, Val f1: 0.7453373074531555\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4898218810558319\n",
            "Train loss: 0.4716959406029094\n",
            "Train loss: 0.4649413627386093\n",
            "Train loss: 0.4620131725695596\n",
            "Train loss: 0.46073085353488014\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5427908971905708, Val f1: 0.8273299336433411\n",
            "Val loss: 0.5271253233606165, Val f1: 0.800871729850769\n",
            "Val loss: 0.5210896211862565, Val f1: 0.7941715717315674\n",
            "Val loss: 0.5181893893142244, Val f1: 0.7893329858779907\n",
            "Val loss: 0.5164099679816336, Val f1: 0.787216305732727\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1252261996269226, Val f1: 1.3415660858154297\n",
            "Val loss: 0.7544710437456766, Val f1: 0.8818177580833435\n",
            "Val loss: 0.6767573475837707, Val f1: 0.8021823167800903\n",
            "Val loss: 0.6461821879659381, Val f1: 0.7640883326530457\n",
            "Val loss: 0.6273891197310554, Val f1: 0.7437753677368164\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.47055680863559246\n",
            "Train loss: 0.45447174978978705\n",
            "Train loss: 0.4490749889612198\n",
            "Train loss: 0.44435165399935705\n",
            "Train loss: 0.44394872585932416\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5264216065406799, Val f1: 0.8452643156051636\n",
            "Val loss: 0.5109162881518855, Val f1: 0.8215963840484619\n",
            "Val loss: 0.5055978369712829, Val f1: 0.8140273690223694\n",
            "Val loss: 0.5031943169992361, Val f1: 0.8101003766059875\n",
            "Val loss: 0.5017059179289001, Val f1: 0.8074913024902344\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1115972399711609, Val f1: 1.3848586082458496\n",
            "Val loss: 0.7454515894254049, Val f1: 0.9038960337638855\n",
            "Val loss: 0.6694494366645813, Val f1: 0.8184154629707336\n",
            "Val loss: 0.639071694442204, Val f1: 0.7771760821342468\n",
            "Val loss: 0.6201658447583517, Val f1: 0.7546557188034058\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.44869537837803364\n",
            "Train loss: 0.4389272720524759\n",
            "Train loss: 0.4328137558698654\n",
            "Train loss: 0.42960351616588993\n",
            "Train loss: 0.4288692960426921\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5189551934599876, Val f1: 0.8494452238082886\n",
            "Val loss: 0.5027878176082264, Val f1: 0.8261317610740662\n",
            "Val loss: 0.49810134053230287, Val f1: 0.8182336091995239\n",
            "Val loss: 0.4960557885134398, Val f1: 0.812528133392334\n",
            "Val loss: 0.49356291478588465, Val f1: 0.8119069337844849\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1057764291763306, Val f1: 1.3784351348876953\n",
            "Val loss: 0.7416980862617493, Val f1: 0.9001273512840271\n",
            "Val loss: 0.6667109489440918, Val f1: 0.8151264190673828\n",
            "Val loss: 0.6365076218332563, Val f1: 0.7749705910682678\n",
            "Val loss: 0.6178084545665317, Val f1: 0.7552698254585266\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.4390820525586605\n",
            "Train loss: 0.422015647093455\n",
            "Train loss: 0.4182205837965012\n",
            "Train loss: 0.4149995330554336\n",
            "Train loss: 0.41322957901727586\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5034001059830189, Val f1: 0.8687246441841125\n",
            "Val loss: 0.4857252456925132, Val f1: 0.8455033302307129\n",
            "Val loss: 0.48173843502998354, Val f1: 0.8357841372489929\n",
            "Val loss: 0.47971293641560114, Val f1: 0.8308072090148926\n",
            "Val loss: 0.47858271109206335, Val f1: 0.8280994296073914\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0973114371299744, Val f1: 1.3936388492584229\n",
            "Val loss: 0.7343859871228536, Val f1: 0.9126156568527222\n",
            "Val loss: 0.659443199634552, Val f1: 0.8257169723510742\n",
            "Val loss: 0.6290417058127267, Val f1: 0.7851607799530029\n",
            "Val loss: 0.6107636955049303, Val f1: 0.763773500919342\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.4156843852251768\n",
            "Train loss: 0.40776906320543\n",
            "Train loss: 0.402478808760643\n",
            "Train loss: 0.39992325119118194\n",
            "Train loss: 0.4005921603668304\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.494290130212903, Val f1: 0.8629944324493408\n",
            "Val loss: 0.4778688432592334, Val f1: 0.8383459448814392\n",
            "Val loss: 0.47152088522911073, Val f1: 0.8326555490493774\n",
            "Val loss: 0.47002838841125144, Val f1: 0.8280412554740906\n",
            "Val loss: 0.46863378548905965, Val f1: 0.8254516124725342\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0896944999694824, Val f1: 1.3769404888153076\n",
            "Val loss: 0.7299765745798746, Val f1: 0.898422122001648\n",
            "Val loss: 0.6565426826477051, Val f1: 0.8142854571342468\n",
            "Val loss: 0.6265624420983451, Val f1: 0.7762497067451477\n",
            "Val loss: 0.6084687047534518, Val f1: 0.7552869915962219\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.4044020064175129\n",
            "Train loss: 0.3931654953595364\n",
            "Train loss: 0.38895422756671905\n",
            "Train loss: 0.38763710811956603\n",
            "Train loss: 0.3877968145977883\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.48136821761727333, Val f1: 0.8875866532325745\n",
            "Val loss: 0.46576581579266174, Val f1: 0.8619937896728516\n",
            "Val loss: 0.46309723019599913, Val f1: 0.8513059020042419\n",
            "Val loss: 0.46053922932539415, Val f1: 0.847926914691925\n",
            "Val loss: 0.459481781792073, Val f1: 0.8455280065536499\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0887645483016968, Val f1: 1.40753173828125\n",
            "Val loss: 0.72797958056132, Val f1: 0.9164294004440308\n",
            "Val loss: 0.6548967957496643, Val f1: 0.8297257423400879\n",
            "Val loss: 0.6251458355358669, Val f1: 0.7904685735702515\n",
            "Val loss: 0.607102460331387, Val f1: 0.7676625847816467\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.3893917575478554\n",
            "Train loss: 0.38034451640013495\n",
            "Train loss: 0.37742828130722045\n",
            "Train loss: 0.376457109824935\n",
            "Train loss: 0.37430538875716074\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.46683892607688904, Val f1: 0.8949999213218689\n",
            "Val loss: 0.45302405501856946, Val f1: 0.8710522055625916\n",
            "Val loss: 0.44949959993362426, Val f1: 0.8622525334358215\n",
            "Val loss: 0.44748932123184204, Val f1: 0.8575789928436279\n",
            "Val loss: 0.446702595267977, Val f1: 0.8535577058792114\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0810490250587463, Val f1: 1.4141607284545898\n",
            "Val loss: 0.7232672770818075, Val f1: 0.9207158088684082\n",
            "Val loss: 0.6507237315177917, Val f1: 0.8343273401260376\n",
            "Val loss: 0.621628395148686, Val f1: 0.7927711606025696\n",
            "Val loss: 0.6031448841094971, Val f1: 0.7705163359642029\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3698504362255335\n",
            "Train loss: 0.3675615019870527\n",
            "Train loss: 0.3654245817661285\n",
            "Train loss: 0.36374189337687707\n",
            "Train loss: 0.3637963327623549\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4631427973508835, Val f1: 0.9004145264625549\n",
            "Val loss: 0.4508390643379905, Val f1: 0.8683089017868042\n",
            "Val loss: 0.44501703798770903, Val f1: 0.8621695041656494\n",
            "Val loss: 0.4428328404675669, Val f1: 0.8561236262321472\n",
            "Val loss: 0.44165838119529544, Val f1: 0.8538042902946472\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0824885368347168, Val f1: 1.4079220294952393\n",
            "Val loss: 0.7246752182642618, Val f1: 0.9145984053611755\n",
            "Val loss: 0.6519785404205323, Val f1: 0.8283678889274597\n",
            "Val loss: 0.6231341872896466, Val f1: 0.7866089344024658\n",
            "Val loss: 0.6045134994718764, Val f1: 0.7631613612174988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "GgB2KbnVHr5E",
        "outputId": "38f15df5-dbda-42bc-f495-b0680652463f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5vbVkPhQQnhJ",
        "outputId": "273af4aa-2f3e-4d6e-c4f1-8cda3cb1e602"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acJ4b6uDemiI"
      },
      "source": [
        "Не сказать, что стало явственно лучше и мы побороли переобучение, но если приглядеться, можно заметить, что bce_loss на 20 эпохах  на валидационной выборке всегда падает, уже нет такого, что оно снова начинает расти, и график заезжает ниже 0.55, а в первом варианте нашей сетки он всегда был выше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IoQDO82F7Qx"
      },
      "source": [
        "## Задача 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2euEjYDfYno",
        "outputId": "3cb8157d-b149-4c2b-962b-39febc6f0081"
      },
      "source": [
        "vocab2 = Counter()\n",
        "for symbol in tweets_data['text']:\n",
        "    vocab2.update(list(symbol))\n",
        "print('всего уникальных символов:', len(vocab2))\n",
        "\n",
        "filtered_vocab2 = set()\n",
        "\n",
        "for symbol in vocab2:\n",
        "    if vocab2[symbol] > 200:\n",
        "        filtered_vocab2.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab2))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 341\n",
            "уникальных символов, втретившихся больше 5 раз: 163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WcsdNbRgcur"
      },
      "source": [
        "symb2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab2:\n",
        "    symb2id[symbol] = len(symb2id)\n",
        "id2symb = {i:symbol for symbol, i in symb2id.items()}\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1BdjAzhthTMV",
        "outputId": "de3fdb0a-d993-4e1f-de5a-0f90f935079d"
      },
      "source": [
        "a = ['as', 'as', 'ds']\n",
        "''.join(a)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'asasds'"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRpXb6QYF9tC"
      },
      "source": [
        "Для задачи 2 мы немного усовершенствуем датасет, чтобы получить две матрицы для разных входов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WREhaLX4GSXX"
      },
      "source": [
        "class TweetsDataset2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symb2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symb2id = symb2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.preprocess(self.dataset[index]) # токенизируем\n",
        "        ids_word = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        tokensstring = ''.join(tokens)\n",
        "        ids_symb = torch.LongTensor([self.symb2id[token] for token in tokensstring if token in self.symb2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids_word, ids_symb, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids_word, ids_symb, y = list(zip(*batch))\n",
        "      padded_ids_word = pad_sequence(ids_word, batch_first=True).to(self.device)\n",
        "      padded_ids_symb = pad_sequence(ids_symb, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_ids_word, padded_ids_symb, y"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwvhatxJLl37"
      },
      "source": [
        "train_dataset = TweetsDataset2(train_sentences, word2id, symb2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHoeF7lYLqsS"
      },
      "source": [
        "val_dataset = TweetsDataset2(val_sentences, word2id, symb2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VeEMbR7LuqM"
      },
      "source": [
        "class CNN2(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size_words, vocab_size_symbs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddingwords = nn.Embedding(vocab_size_words, 100)\n",
        "        self.embeddingwords.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.emb2h = nn.Linear(100, 10)\n",
        "        \n",
        "        self.embeddingsymbs = nn.Embedding(vocab_size_symbs, 10)\n",
        "        self.bigrams = nn.Conv1d(in_channels=10, out_channels=60, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=10, out_channels=30, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text_words, text_symbs):\n",
        "        #cначала слова\n",
        "        embeddedwords = self.embeddingwords(text_words)\n",
        "        mean_emb = torch.mean(embeddedwords, dim=1)\n",
        "        X1 = self.dropout(self.relu(self.emb2h(mean_emb)))\n",
        "        #теперь символы\n",
        "        embeddedsymbs = self.embeddingsymbs(text_symbs)\n",
        "        embeddedsymbs = embeddedsymbs.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embeddedsymbs))))\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embeddedsymbs))))\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2, X1), 1)\n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits\n",
        "      "
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1aB63B8oqFN"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (text_words, text_symbs, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(text_words, text_symbs)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (text_words, text_symbs, ys) in enumerate(iterator):   \n",
        "            preds = model(text_words, text_symbs)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHB-js-4pIOo",
        "outputId": "bfec0ff6-8a1c-4f89-9696-efc29d750087"
      },
      "source": [
        "model = CNN2(len(word2id),len(symb2id))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(50):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.7501299418509007\n",
            "Train loss: 0.7206947803497314\n",
            "Train loss: 0.7080442380905151\n",
            "Train loss: 0.6997053133907603\n",
            "Train loss: 0.6930946970269793\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7147233709692955, Val f1: 0.7158839702606201\n",
            "Val loss: 0.6935645432183237, Val f1: 0.6937904357910156\n",
            "Val loss: 0.68652046084404, Val f1: 0.6859506368637085\n",
            "Val loss: 0.6828919942699262, Val f1: 0.6823459267616272\n",
            "Val loss: 0.6805408469268254, Val f1: 0.6809853315353394\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3382514715194702, Val f1: 1.3725643157958984\n",
            "Val loss: 0.8990925749142965, Val f1: 0.8891474008560181\n",
            "Val loss: 0.8096268773078918, Val f1: 0.7992087602615356\n",
            "Val loss: 0.7703493748392377, Val f1: 0.7652981877326965\n",
            "Val loss: 0.7485297057363722, Val f1: 0.7456901669502258\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.7043457254767418\n",
            "Train loss: 0.6801320260221307\n",
            "Train loss: 0.6718469691276551\n",
            "Train loss: 0.6659466001524854\n",
            "Train loss: 0.6615996410449346\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6941615119576454, Val f1: 0.7364970445632935\n",
            "Val loss: 0.673219848762859, Val f1: 0.7113015651702881\n",
            "Val loss: 0.6668648791313171, Val f1: 0.7034650444984436\n",
            "Val loss: 0.6645166998478904, Val f1: 0.6972124576568604\n",
            "Val loss: 0.6626329585200265, Val f1: 0.6943373680114746\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3003923892974854, Val f1: 1.3875420093536377\n",
            "Val loss: 0.8761642575263977, Val f1: 0.9036848545074463\n",
            "Val loss: 0.7899219036102295, Val f1: 0.8101348876953125\n",
            "Val loss: 0.7506426998547145, Val f1: 0.7762185335159302\n",
            "Val loss: 0.7292197346687317, Val f1: 0.7569659352302551\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6809112727642059\n",
            "Train loss: 0.6597385930292534\n",
            "Train loss: 0.6513263356685638\n",
            "Train loss: 0.6455565602032106\n",
            "Train loss: 0.6423776880616233\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6780197769403458, Val f1: 0.7491573095321655\n",
            "Val loss: 0.6596444249153137, Val f1: 0.7221309542655945\n",
            "Val loss: 0.6526776599884033, Val f1: 0.7162688970565796\n",
            "Val loss: 0.649124737995774, Val f1: 0.7135939002037048\n",
            "Val loss: 0.647243714758328, Val f1: 0.7113913893699646\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2721662521362305, Val f1: 1.4138203859329224\n",
            "Val loss: 0.8579632441202799, Val f1: 0.9250966906547546\n",
            "Val loss: 0.7737586498260498, Val f1: 0.8277149200439453\n",
            "Val loss: 0.7341510908944267, Val f1: 0.7939262986183167\n",
            "Val loss: 0.7129256923993429, Val f1: 0.7761585712432861\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6633468940854073\n",
            "Train loss: 0.6428735689683394\n",
            "Train loss: 0.6332259559631348\n",
            "Train loss: 0.6290055148637117\n",
            "Train loss: 0.6250549334855307\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6606268920004368, Val f1: 0.7628253698348999\n",
            "Val loss: 0.6411922212803003, Val f1: 0.7384840846061707\n",
            "Val loss: 0.6351012146472931, Val f1: 0.7305383086204529\n",
            "Val loss: 0.6317255158922566, Val f1: 0.7267382144927979\n",
            "Val loss: 0.629624292964027, Val f1: 0.725441575050354\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.242614507675171, Val f1: 1.4355472326278687\n",
            "Val loss: 0.8373133937517802, Val f1: 0.9451611042022705\n",
            "Val loss: 0.7551965832710266, Val f1: 0.846354603767395\n",
            "Val loss: 0.7156448449407306, Val f1: 0.8124483823776245\n",
            "Val loss: 0.6947496069802178, Val f1: 0.793774425983429\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.646070409566164\n",
            "Train loss: 0.6246878865993384\n",
            "Train loss: 0.6159634685516358\n",
            "Train loss: 0.6104884120955396\n",
            "Train loss: 0.6068442109085265\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.638234481215477, Val f1: 0.7799695730209351\n",
            "Val loss: 0.6207383654334329, Val f1: 0.7514963746070862\n",
            "Val loss: 0.6136408913135528, Val f1: 0.7451180219650269\n",
            "Val loss: 0.6098323055167696, Val f1: 0.7418608665466309\n",
            "Val loss: 0.6084008231049493, Val f1: 0.7394012808799744\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2095769047737122, Val f1: 1.438236117362976\n",
            "Val loss: 0.8135833144187927, Val f1: 0.9559386968612671\n",
            "Val loss: 0.7334946632385254, Val f1: 0.8585848212242126\n",
            "Val loss: 0.6942355547632489, Val f1: 0.8259317874908447\n",
            "Val loss: 0.6738189591301812, Val f1: 0.806049644947052\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6255851089954376\n",
            "Train loss: 0.603821129509897\n",
            "Train loss: 0.5949620592594147\n",
            "Train loss: 0.5899743126399482\n",
            "Train loss: 0.5870459562256223\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6114085763692856, Val f1: 0.7875880599021912\n",
            "Val loss: 0.5931586587067806, Val f1: 0.7640560269355774\n",
            "Val loss: 0.586273649930954, Val f1: 0.7561028599739075\n",
            "Val loss: 0.5839517837140098, Val f1: 0.7517461180686951\n",
            "Val loss: 0.5822788029909134, Val f1: 0.7501559853553772\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1683982610702515, Val f1: 1.440688133239746\n",
            "Val loss: 0.7839187184969584, Val f1: 0.9655446410179138\n",
            "Val loss: 0.7068616628646851, Val f1: 0.8661247491836548\n",
            "Val loss: 0.6685311964579991, Val f1: 0.8319904208183289\n",
            "Val loss: 0.6488295396169027, Val f1: 0.8110458254814148\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6010690703988075\n",
            "Train loss: 0.5827216379570238\n",
            "Train loss: 0.5768837821483612\n",
            "Train loss: 0.5717202158116582\n",
            "Train loss: 0.5679216505516143\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5882274620234966, Val f1: 0.8004282116889954\n",
            "Val loss: 0.570346398787065, Val f1: 0.7760939598083496\n",
            "Val loss: 0.5634431207180023, Val f1: 0.7690069675445557\n",
            "Val loss: 0.5611109288770761, Val f1: 0.763993501663208\n",
            "Val loss: 0.5587396870056788, Val f1: 0.7621133327484131\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1316677331924438, Val f1: 1.4459631443023682\n",
            "Val loss: 0.7581966519355774, Val f1: 0.9767140746116638\n",
            "Val loss: 0.6840751647949219, Val f1: 0.8740377426147461\n",
            "Val loss: 0.6468213966914585, Val f1: 0.8403101563453674\n",
            "Val loss: 0.627695984310574, Val f1: 0.8192991018295288\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5836817435920238\n",
            "Train loss: 0.5619419506101897\n",
            "Train loss: 0.5543960320949555\n",
            "Train loss: 0.5513281733242433\n",
            "Train loss: 0.548071258124851\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5562513768672943, Val f1: 0.818500816822052\n",
            "Val loss: 0.5419197100581545, Val f1: 0.792206883430481\n",
            "Val loss: 0.53799973487854, Val f1: 0.7807116508483887\n",
            "Val loss: 0.5352501228674135, Val f1: 0.77760249376297\n",
            "Val loss: 0.534413510844821, Val f1: 0.7747805714607239\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0957033038139343, Val f1: 1.4752318859100342\n",
            "Val loss: 0.7334649761517843, Val f1: 0.988868236541748\n",
            "Val loss: 0.6623739004135132, Val f1: 0.8845449686050415\n",
            "Val loss: 0.6262844971248082, Val f1: 0.8488991856575012\n",
            "Val loss: 0.6077889535162184, Val f1: 0.827166736125946\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5609966889023781\n",
            "Train loss: 0.5440080346483173\n",
            "Train loss: 0.5363710176944733\n",
            "Train loss: 0.5334437178142035\n",
            "Train loss: 0.5297698587888763\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5390071999281645, Val f1: 0.8204428553581238\n",
            "Val loss: 0.5228263166817751, Val f1: 0.7974711656570435\n",
            "Val loss: 0.517659209370613, Val f1: 0.790054202079773\n",
            "Val loss: 0.5149030027104847, Val f1: 0.7869213223457336\n",
            "Val loss: 0.512738993480092, Val f1: 0.7855172753334045\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0628716945648193, Val f1: 1.5006306171417236\n",
            "Val loss: 0.7116188406944275, Val f1: 1.001145839691162\n",
            "Val loss: 0.6433439135551453, Val f1: 0.8929979205131531\n",
            "Val loss: 0.6085952009473529, Val f1: 0.8550530672073364\n",
            "Val loss: 0.5905813111199273, Val f1: 0.8333491086959839\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5455621611326933\n",
            "Train loss: 0.5247365832328796\n",
            "Train loss: 0.5179573035240174\n",
            "Train loss: 0.5139241712306862\n",
            "Train loss: 0.5114634523079509\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5202196408063173, Val f1: 0.8308667540550232\n",
            "Val loss: 0.5024237343759248, Val f1: 0.8124309182167053\n",
            "Val loss: 0.49898562371730804, Val f1: 0.8034769296646118\n",
            "Val loss: 0.4960666761469485, Val f1: 0.7993722558021545\n",
            "Val loss: 0.494856809931142, Val f1: 0.7977689504623413\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.041085422039032, Val f1: 1.520289659500122\n",
            "Val loss: 0.6973615090052286, Val f1: 1.0115535259246826\n",
            "Val loss: 0.6309046626091004, Val f1: 0.902661144733429\n",
            "Val loss: 0.5965873513902936, Val f1: 0.8635408878326416\n",
            "Val loss: 0.5786708858278062, Val f1: 0.8412631154060364\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.5243666227906942\n",
            "Train loss: 0.5045511379386439\n",
            "Train loss: 0.49998105943202975\n",
            "Train loss: 0.4968511804715911\n",
            "Train loss: 0.49518435696760815\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5085680149495602, Val f1: 0.842090368270874\n",
            "Val loss: 0.4911071296894189, Val f1: 0.8190046548843384\n",
            "Val loss: 0.484675714969635, Val f1: 0.8127856850624084\n",
            "Val loss: 0.4819452900495102, Val f1: 0.8090378046035767\n",
            "Val loss: 0.47999035602524165, Val f1: 0.8071799874305725\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0221248269081116, Val f1: 1.5207867622375488\n",
            "Val loss: 0.6856927076975504, Val f1: 1.0133252143859863\n",
            "Val loss: 0.620949125289917, Val f1: 0.9054109454154968\n",
            "Val loss: 0.5872733422688076, Val f1: 0.8669611215591431\n",
            "Val loss: 0.5693183441956838, Val f1: 0.8446827530860901\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.5090122930705547\n",
            "Train loss: 0.49495502583908313\n",
            "Train loss: 0.48926750004291536\n",
            "Train loss: 0.4848323233092009\n",
            "Train loss: 0.4820995628833771\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.49118887074291706, Val f1: 0.8502287268638611\n",
            "Val loss: 0.47470166556762927, Val f1: 0.8251347541809082\n",
            "Val loss: 0.46638189256191254, Val f1: 0.8196163773536682\n",
            "Val loss: 0.4626683165777975, Val f1: 0.8174422979354858\n",
            "Val loss: 0.46087134984277545, Val f1: 0.8156324028968811\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9977551102638245, Val f1: 1.534510850906372\n",
            "Val loss: 0.669961005449295, Val f1: 1.0222554206848145\n",
            "Val loss: 0.6076753556728363, Val f1: 0.9095382690429688\n",
            "Val loss: 0.5749688531671252, Val f1: 0.8708455562591553\n",
            "Val loss: 0.5574387808640798, Val f1: 0.8474041819572449\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.49998127296566963\n",
            "Train loss: 0.4843138347972523\n",
            "Train loss: 0.47727991759777066\n",
            "Train loss: 0.47428850673917516\n",
            "Train loss: 0.4707285999542191\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4694334343075752, Val f1: 0.864849865436554\n",
            "Val loss: 0.45612822337584064, Val f1: 0.8396967649459839\n",
            "Val loss: 0.4544688498973846, Val f1: 0.8285951614379883\n",
            "Val loss: 0.45296902443046, Val f1: 0.824829638004303\n",
            "Val loss: 0.45183868351436796, Val f1: 0.8217529058456421\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9943568706512451, Val f1: 1.53374183177948\n",
            "Val loss: 0.6684624552726746, Val f1: 1.0199329853057861\n",
            "Val loss: 0.6063898742198944, Val f1: 0.9092880487442017\n",
            "Val loss: 0.5734613963535854, Val f1: 0.8720794916152954\n",
            "Val loss: 0.5553843047883775, Val f1: 0.8504032492637634\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.4891465753316879\n",
            "Train loss: 0.4693990232366504\n",
            "Train loss: 0.4629016762971878\n",
            "Train loss: 0.4597022871472942\n",
            "Train loss: 0.45776674719083876\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4580234382301569, Val f1: 0.8681676983833313\n",
            "Val loss: 0.44450178110238275, Val f1: 0.8411564230918884\n",
            "Val loss: 0.43999478340148923, Val f1: 0.8338316679000854\n",
            "Val loss: 0.4378215114572155, Val f1: 0.8297953605651855\n",
            "Val loss: 0.4353157402504058, Val f1: 0.828627347946167\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9736694097518921, Val f1: 1.5363112688064575\n",
            "Val loss: 0.6550668676694235, Val f1: 1.0220482349395752\n",
            "Val loss: 0.595043021440506, Val f1: 0.9097423553466797\n",
            "Val loss: 0.5629815033503941, Val f1: 0.8717623949050903\n",
            "Val loss: 0.5451302395926582, Val f1: 0.8489115834236145\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.4705060478299856\n",
            "Train loss: 0.45344641985315265\n",
            "Train loss: 0.4489045989513397\n",
            "Train loss: 0.44766977918681816\n",
            "Train loss: 0.4479937493091538\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4424302987754345, Val f1: 0.8776698112487793\n",
            "Val loss: 0.43127251213247125, Val f1: 0.84797203540802\n",
            "Val loss: 0.42676332235336306, Val f1: 0.8401044011116028\n",
            "Val loss: 0.4246190083560659, Val f1: 0.834544837474823\n",
            "Val loss: 0.42314456190381733, Val f1: 0.8320619463920593\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.954616367816925, Val f1: 1.5317647457122803\n",
            "Val loss: 0.6431036094824473, Val f1: 1.022172451019287\n",
            "Val loss: 0.5851978242397309, Val f1: 0.9057862162590027\n",
            "Val loss: 0.5545646207673209, Val f1: 0.8658116459846497\n",
            "Val loss: 0.5372561911741892, Val f1: 0.8413103222846985\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.4631620664149523\n",
            "Train loss: 0.4493848311178612\n",
            "Train loss: 0.443368644118309\n",
            "Train loss: 0.43989507386933513\n",
            "Train loss: 0.43830648951587226\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43726789206266403, Val f1: 0.8810408115386963\n",
            "Val loss: 0.4277155309012442, Val f1: 0.8521463871002197\n",
            "Val loss: 0.4221661669015884, Val f1: 0.8447084426879883\n",
            "Val loss: 0.41992372719209586, Val f1: 0.8404343724250793\n",
            "Val loss: 0.418745325079986, Val f1: 0.8382410407066345\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9666701853275299, Val f1: 1.5528063774108887\n",
            "Val loss: 0.6508959333101908, Val f1: 1.029282569885254\n",
            "Val loss: 0.5914768099784851, Val f1: 0.9150088429450989\n",
            "Val loss: 0.5595131644180843, Val f1: 0.8752989768981934\n",
            "Val loss: 0.5410239067342546, Val f1: 0.853786289691925\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.44800427556037903\n",
            "Train loss: 0.43616894790620514\n",
            "Train loss: 0.4327369010448456\n",
            "Train loss: 0.43181097774363275\n",
            "Train loss: 0.4300507516378448\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42899742908775806, Val f1: 0.8898618221282959\n",
            "Val loss: 0.4144930902755622, Val f1: 0.8616043329238892\n",
            "Val loss: 0.41132131397724153, Val f1: 0.8524096608161926\n",
            "Val loss: 0.40956887069033154, Val f1: 0.847258985042572\n",
            "Val loss: 0.40857402093353723, Val f1: 0.8441843390464783\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9608869254589081, Val f1: 1.5554916858673096\n",
            "Val loss: 0.6474156181017557, Val f1: 1.0316990613937378\n",
            "Val loss: 0.5891118109226227, Val f1: 0.917373776435852\n",
            "Val loss: 0.557287301336016, Val f1: 0.8773619532585144\n",
            "Val loss: 0.5386773182286156, Val f1: 0.8550152778625488\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.4386953841894865\n",
            "Train loss: 0.42831170017069037\n",
            "Train loss: 0.4250751006603241\n",
            "Train loss: 0.4213340936312035\n",
            "Train loss: 0.4199292574610029\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4152932930737734, Val f1: 0.8931700587272644\n",
            "Val loss: 0.4064229699698361, Val f1: 0.8624186515808105\n",
            "Val loss: 0.40348677337169647, Val f1: 0.8534688949584961\n",
            "Val loss: 0.4017603299511013, Val f1: 0.8503305315971375\n",
            "Val loss: 0.40219268876881825, Val f1: 0.846845805644989\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9611740112304688, Val f1: 1.5498435497283936\n",
            "Val loss: 0.6478322843710581, Val f1: 1.0297569036483765\n",
            "Val loss: 0.5895710945129394, Val f1: 0.9185091257095337\n",
            "Val loss: 0.557422229221889, Val f1: 0.8785751461982727\n",
            "Val loss: 0.5386390255557166, Val f1: 0.855439305305481\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.43400268256664276\n",
            "Train loss: 0.4199854650280692\n",
            "Train loss: 0.41588214874267576\n",
            "Train loss: 0.4131525452457257\n",
            "Train loss: 0.4116757001195635\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40901146456599236, Val f1: 0.89306640625\n",
            "Val loss: 0.39650446989319543, Val f1: 0.8670567870140076\n",
            "Val loss: 0.3933538764715195, Val f1: 0.858426570892334\n",
            "Val loss: 0.3902691166792343, Val f1: 0.8553786277770996\n",
            "Val loss: 0.38890514097043444, Val f1: 0.8529331684112549\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9493301808834076, Val f1: 1.5467126369476318\n",
            "Val loss: 0.6404890716075897, Val f1: 1.0270309448242188\n",
            "Val loss: 0.5839762091636658, Val f1: 0.9136951565742493\n",
            "Val loss: 0.5522298557417733, Val f1: 0.8756537437438965\n",
            "Val loss: 0.5337583223978678, Val f1: 0.8526820540428162\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.42168264649808407\n",
            "Train loss: 0.4079903309995478\n",
            "Train loss: 0.4056689590215683\n",
            "Train loss: 0.40378371459334644\n",
            "Train loss: 0.4030116714891933\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39981886371970177, Val f1: 0.9028189778327942\n",
            "Val loss: 0.387064255548246, Val f1: 0.8734127283096313\n",
            "Val loss: 0.3829502260684967, Val f1: 0.865024983882904\n",
            "Val loss: 0.3808961835370135, Val f1: 0.8598659038543701\n",
            "Val loss: 0.3797527980946359, Val f1: 0.8579087257385254\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9422147274017334, Val f1: 1.5554704666137695\n",
            "Val loss: 0.6361898978551229, Val f1: 1.0325143337249756\n",
            "Val loss: 0.580468761920929, Val f1: 0.9173990488052368\n",
            "Val loss: 0.5492274676050458, Val f1: 0.8775776624679565\n",
            "Val loss: 0.5309185915523105, Val f1: 0.8547669649124146\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.41772005893290043\n",
            "Train loss: 0.4045812198610017\n",
            "Train loss: 0.4000260293483734\n",
            "Train loss: 0.39755015853625625\n",
            "Train loss: 0.39605568526756196\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38920153118669987, Val f1: 0.9023323059082031\n",
            "Val loss: 0.37698759815909644, Val f1: 0.8769189715385437\n",
            "Val loss: 0.3734279978275299, Val f1: 0.868373692035675\n",
            "Val loss: 0.370373836648998, Val f1: 0.8657510876655579\n",
            "Val loss: 0.3707284703850746, Val f1: 0.8617076873779297\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9367658793926239, Val f1: 1.5479909181594849\n",
            "Val loss: 0.6329646309216818, Val f1: 1.0275282859802246\n",
            "Val loss: 0.5782565593719482, Val f1: 0.9161298871040344\n",
            "Val loss: 0.5475068773542132, Val f1: 0.8767504692077637\n",
            "Val loss: 0.5294387870364718, Val f1: 0.853123664855957\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.4110312517732382\n",
            "Train loss: 0.39653075644464203\n",
            "Train loss: 0.3909885710477829\n",
            "Train loss: 0.39074738434891204\n",
            "Train loss: 0.3902765452152207\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3871887978166342, Val f1: 0.9062427282333374\n",
            "Val loss: 0.3729753629727797, Val f1: 0.8815129995346069\n",
            "Val loss: 0.36937438547611234, Val f1: 0.8714670538902283\n",
            "Val loss: 0.3677768306945687, Val f1: 0.8671936392784119\n",
            "Val loss: 0.3666358373704411, Val f1: 0.8650675415992737\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9437052011489868, Val f1: 1.551741361618042\n",
            "Val loss: 0.6375796397527059, Val f1: 1.0317120552062988\n",
            "Val loss: 0.5822497844696045, Val f1: 0.9200769662857056\n",
            "Val loss: 0.5508270817143577, Val f1: 0.8802307844161987\n",
            "Val loss: 0.5323344899548424, Val f1: 0.856619119644165\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.4055602718144655\n",
            "Train loss: 0.39435717373183277\n",
            "Train loss: 0.3887933051586151\n",
            "Train loss: 0.38448027665935347\n",
            "Train loss: 0.3845651880616233\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3807814344763756, Val f1: 0.9126840829849243\n",
            "Val loss: 0.367005649841193, Val f1: 0.8846793174743652\n",
            "Val loss: 0.3636000782251358, Val f1: 0.8759057521820068\n",
            "Val loss: 0.36161641309510417, Val f1: 0.8715612292289734\n",
            "Val loss: 0.36107025898638223, Val f1: 0.8688678741455078\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9372374713420868, Val f1: 1.551363229751587\n",
            "Val loss: 0.6337807377179464, Val f1: 1.030914545059204\n",
            "Val loss: 0.5789081811904907, Val f1: 0.9185507893562317\n",
            "Val loss: 0.5477211645671299, Val f1: 0.8792614936828613\n",
            "Val loss: 0.529513935248057, Val f1: 0.8552961945533752\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.403213107958436\n",
            "Train loss: 0.3870519751852209\n",
            "Train loss: 0.3827825802564621\n",
            "Train loss: 0.3808700735889264\n",
            "Train loss: 0.37977584309521173\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3701053261756897, Val f1: 0.9196246862411499\n",
            "Val loss: 0.36024416215492017, Val f1: 0.8888018131256104\n",
            "Val loss: 0.35693966686725614, Val f1: 0.8789660930633545\n",
            "Val loss: 0.3567505318727066, Val f1: 0.8742050528526306\n",
            "Val loss: 0.3558139240457898, Val f1: 0.8710544109344482\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9453804194927216, Val f1: 1.5557959079742432\n",
            "Val loss: 0.6395605504512787, Val f1: 1.0299195051193237\n",
            "Val loss: 0.5845754086971283, Val f1: 0.9185435175895691\n",
            "Val loss: 0.5528033929211753, Val f1: 0.8801825642585754\n",
            "Val loss: 0.5341962112320794, Val f1: 0.8564379811286926\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.3920294772833586\n",
            "Train loss: 0.38127812923807086\n",
            "Train loss: 0.37851274132728574\n",
            "Train loss: 0.3751394633036941\n",
            "Train loss: 0.37375046915951227\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3716986048966646, Val f1: 0.9154853820800781\n",
            "Val loss: 0.358154145154086, Val f1: 0.8894728422164917\n",
            "Val loss: 0.35194417476654055, Val f1: 0.8815451860427856\n",
            "Val loss: 0.3513476079079642, Val f1: 0.876067042350769\n",
            "Val loss: 0.3500952695806821, Val f1: 0.8733916878700256\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.950253427028656, Val f1: 1.5554600954055786\n",
            "Val loss: 0.6424504915873209, Val f1: 1.0303423404693604\n",
            "Val loss: 0.5877545356750489, Val f1: 0.9190459251403809\n",
            "Val loss: 0.5556611035551343, Val f1: 0.8807640671730042\n",
            "Val loss: 0.5369947685135735, Val f1: 0.8574360609054565\n",
            "\n",
            "starting Epoch 25\n",
            "Training...\n",
            "Train loss: 0.38031788915395737\n",
            "Train loss: 0.37032851667115185\n",
            "Train loss: 0.3675236350297928\n",
            "Train loss: 0.3667714782615206\n",
            "Train loss: 0.36790407981191364\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36009148322045803, Val f1: 0.9216231107711792\n",
            "Val loss: 0.34964523261243646, Val f1: 0.8930768966674805\n",
            "Val loss: 0.3467192959785461, Val f1: 0.8833335041999817\n",
            "Val loss: 0.34464197772652355, Val f1: 0.8797558546066284\n",
            "Val loss: 0.34315820109276546, Val f1: 0.8772154450416565\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.940877377986908, Val f1: 1.553719162940979\n",
            "Val loss: 0.6359220345815023, Val f1: 1.0290617942810059\n",
            "Val loss: 0.5817769408226013, Val f1: 0.9179151654243469\n",
            "Val loss: 0.5505615259919848, Val f1: 0.8785845637321472\n",
            "Val loss: 0.5322681930330064, Val f1: 0.8551813960075378\n",
            "\n",
            "starting Epoch 26\n",
            "Training...\n",
            "Train loss: 0.37675624899566174\n",
            "Train loss: 0.37103956124999304\n",
            "Train loss: 0.3674047440290451\n",
            "Train loss: 0.3642698922264042\n",
            "Train loss: 0.36274614539884387\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.35086297802627087, Val f1: 0.9227506518363953\n",
            "Val loss: 0.3452695644263065, Val f1: 0.8930964469909668\n",
            "Val loss: 0.3392797946929932, Val f1: 0.8847818374633789\n",
            "Val loss: 0.3378102405747371, Val f1: 0.8807589411735535\n",
            "Val loss: 0.33610324490638005, Val f1: 0.8789771795272827\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.958156943321228, Val f1: 1.552807331085205\n",
            "Val loss: 0.6474970976511637, Val f1: 1.0295054912567139\n",
            "Val loss: 0.5926672339439392, Val f1: 0.9188984036445618\n",
            "Val loss: 0.560204531465258, Val f1: 0.8799084424972534\n",
            "Val loss: 0.5415782531102499, Val f1: 0.8562291264533997\n",
            "\n",
            "starting Epoch 27\n",
            "Training...\n",
            "Train loss: 0.3820403218269348\n",
            "Train loss: 0.36606346748091956\n",
            "Train loss: 0.36317880392074586\n",
            "Train loss: 0.35894828634475595\n",
            "Train loss: 0.3583920842834881\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3604130744934082, Val f1: 0.9254085421562195\n",
            "Val loss: 0.3503468551418998, Val f1: 0.8947979807853699\n",
            "Val loss: 0.34634368121623993, Val f1: 0.885403573513031\n",
            "Val loss: 0.3436119801072932, Val f1: 0.8811447024345398\n",
            "Val loss: 0.34050898750623065, Val f1: 0.8794898986816406\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9679479002952576, Val f1: 1.5575522184371948\n",
            "Val loss: 0.653942346572876, Val f1: 1.0337066650390625\n",
            "Val loss: 0.597860598564148, Val f1: 0.9246875643730164\n",
            "Val loss: 0.56485744033541, Val f1: 0.8851979970932007\n",
            "Val loss: 0.5456571578979492, Val f1: 0.8599300980567932\n",
            "\n",
            "starting Epoch 28\n",
            "Training...\n",
            "Train loss: 0.36614655889570713\n",
            "Train loss: 0.35690584417545435\n",
            "Train loss: 0.3544474244117737\n",
            "Train loss: 0.35247540296013674\n",
            "Train loss: 0.35241939021008356\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3497445471584797, Val f1: 0.9238007068634033\n",
            "Val loss: 0.3391731923276728, Val f1: 0.8966211080551147\n",
            "Val loss: 0.3361390608549118, Val f1: 0.8879354596138\n",
            "Val loss: 0.3352752528083858, Val f1: 0.8828326463699341\n",
            "Val loss: 0.33522845130591167, Val f1: 0.8797054290771484\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9808664917945862, Val f1: 1.5564627647399902\n",
            "Val loss: 0.6625738243261973, Val f1: 1.0315877199172974\n",
            "Val loss: 0.6062208473682403, Val f1: 0.9225350618362427\n",
            "Val loss: 0.5724836162158421, Val f1: 0.8840900659561157\n",
            "Val loss: 0.5530150863859389, Val f1: 0.8594954609870911\n",
            "\n",
            "starting Epoch 29\n",
            "Training...\n",
            "Train loss: 0.36543250270187855\n",
            "Train loss: 0.3575518022884022\n",
            "Train loss: 0.35368799805641177\n",
            "Train loss: 0.35018923033529253\n",
            "Train loss: 0.34975034175884157\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3378887455910444, Val f1: 0.9328812956809998\n",
            "Val loss: 0.3301371299859249, Val f1: 0.9020212292671204\n",
            "Val loss: 0.3256469404697418, Val f1: 0.8945651054382324\n",
            "Val loss: 0.32339484522591777, Val f1: 0.8898378610610962\n",
            "Val loss: 0.3224346031035696, Val f1: 0.8874646425247192\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.957015335559845, Val f1: 1.5511705875396729\n",
            "Val loss: 0.6467422942320505, Val f1: 1.0292363166809082\n",
            "Val loss: 0.593044537305832, Val f1: 0.9187566637992859\n",
            "Val loss: 0.5607662286077227, Val f1: 0.879996657371521\n",
            "Val loss: 0.5424884590837691, Val f1: 0.8563745617866516\n",
            "\n",
            "starting Epoch 30\n",
            "Training...\n",
            "Train loss: 0.36000640504062176\n",
            "Train loss: 0.3475634112502589\n",
            "Train loss: 0.3466764396429062\n",
            "Train loss: 0.3463204027111851\n",
            "Train loss: 0.3462019853648685\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.33768931590020657, Val f1: 0.9320662021636963\n",
            "Val loss: 0.3223993471174529, Val f1: 0.9059039950370789\n",
            "Val loss: 0.3201982480287552, Val f1: 0.8971412181854248\n",
            "Val loss: 0.31974498104693283, Val f1: 0.8922262787818909\n",
            "Val loss: 0.3184520974755287, Val f1: 0.8899639844894409\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9536153674125671, Val f1: 1.548622488975525\n",
            "Val loss: 0.6441368957360586, Val f1: 1.0272190570831299\n",
            "Val loss: 0.5908875405788422, Val f1: 0.9167303442955017\n",
            "Val loss: 0.5591865309647152, Val f1: 0.8773801922798157\n",
            "Val loss: 0.5410955713854896, Val f1: 0.8546071648597717\n",
            "\n",
            "starting Epoch 31\n",
            "Training...\n",
            "Train loss: 0.3525311350822449\n",
            "Train loss: 0.34761693802746857\n",
            "Train loss: 0.3428132110834122\n",
            "Train loss: 0.3406060827312185\n",
            "Train loss: 0.340796855588754\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3304671533405781, Val f1: 0.9372072219848633\n",
            "Val loss: 0.322025275591648, Val f1: 0.9061332941055298\n",
            "Val loss: 0.3175298011302948, Val f1: 0.8986206650733948\n",
            "Val loss: 0.31568131651451337, Val f1: 0.8942393660545349\n",
            "Val loss: 0.31481297101293293, Val f1: 0.891753613948822\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.961340606212616, Val f1: 1.5461034774780273\n",
            "Val loss: 0.6486954391002655, Val f1: 1.0277996063232422\n",
            "Val loss: 0.5952198207378387, Val f1: 0.9190004467964172\n",
            "Val loss: 0.5630980517183032, Val f1: 0.8798202276229858\n",
            "Val loss: 0.5448128018114302, Val f1: 0.8555252552032471\n",
            "\n",
            "starting Epoch 32\n",
            "Training...\n",
            "Train loss: 0.35506314039230347\n",
            "Train loss: 0.34002021767876367\n",
            "Train loss: 0.3398596829175949\n",
            "Train loss: 0.33815210790776495\n",
            "Train loss: 0.33717703890232814\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3283492885529995, Val f1: 0.9360284805297852\n",
            "Val loss: 0.3151723697330012, Val f1: 0.9093778729438782\n",
            "Val loss: 0.3132593840360641, Val f1: 0.8991397023200989\n",
            "Val loss: 0.31190401582575555, Val f1: 0.8944884538650513\n",
            "Val loss: 0.3109694006187575, Val f1: 0.8921915888786316\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9846371412277222, Val f1: 1.54441499710083\n",
            "Val loss: 0.6634489496548971, Val f1: 1.0260080099105835\n",
            "Val loss: 0.608847188949585, Val f1: 0.9186363220214844\n",
            "Val loss: 0.5755992020879473, Val f1: 0.879010796546936\n",
            "Val loss: 0.556944857041041, Val f1: 0.8548300862312317\n",
            "\n",
            "starting Epoch 33\n",
            "Training...\n",
            "Train loss: 0.3511773981153965\n",
            "Train loss: 0.3425592238252813\n",
            "Train loss: 0.3362711030244827\n",
            "Train loss: 0.33286409947409556\n",
            "Train loss: 0.3323109217342876\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.32859073393046856, Val f1: 0.9407728910446167\n",
            "Val loss: 0.31533798214161035, Val f1: 0.9117663502693176\n",
            "Val loss: 0.310521115064621, Val f1: 0.9029805064201355\n",
            "Val loss: 0.3083503010557659, Val f1: 0.8982273936271667\n",
            "Val loss: 0.3073618734876315, Val f1: 0.8950450420379639\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.990095853805542, Val f1: 1.5520920753479004\n",
            "Val loss: 0.6674650212128957, Val f1: 1.027064323425293\n",
            "Val loss: 0.6129989445209503, Val f1: 0.9204406142234802\n",
            "Val loss: 0.5797164482729775, Val f1: 0.8809869289398193\n",
            "Val loss: 0.5607810318470001, Val f1: 0.8565587997436523\n",
            "\n",
            "starting Epoch 34\n",
            "Training...\n",
            "Train loss: 0.3422442153096199\n",
            "Train loss: 0.3346769285924507\n",
            "Train loss: 0.33059688746929167\n",
            "Train loss: 0.3305514730624299\n",
            "Train loss: 0.3288869134017399\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3166984133422375, Val f1: 0.9435240030288696\n",
            "Val loss: 0.30632710366538074, Val f1: 0.9136949181556702\n",
            "Val loss: 0.30114055454730987, Val f1: 0.905920684337616\n",
            "Val loss: 0.29919827562659534, Val f1: 0.9014407396316528\n",
            "Val loss: 0.29758321316469283, Val f1: 0.8988187313079834\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9863097071647644, Val f1: 1.545835256576538\n",
            "Val loss: 0.6652768353621165, Val f1: 1.0249049663543701\n",
            "Val loss: 0.6120791375637055, Val f1: 0.9165065884590149\n",
            "Val loss: 0.5791096218994686, Val f1: 0.8771766424179077\n",
            "Val loss: 0.5607883897092607, Val f1: 0.8525565266609192\n",
            "\n",
            "starting Epoch 35\n",
            "Training...\n",
            "Train loss: 0.3506013974547386\n",
            "Train loss: 0.3356694362380288\n",
            "Train loss: 0.3281778746843338\n",
            "Train loss: 0.32755090941244097\n",
            "Train loss: 0.32633001038006376\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.31477327086031437, Val f1: 0.9452286958694458\n",
            "Val loss: 0.3042427445902969, Val f1: 0.9166746139526367\n",
            "Val loss: 0.30117381274700167, Val f1: 0.9065971374511719\n",
            "Val loss: 0.30003348393226736, Val f1: 0.9016299247741699\n",
            "Val loss: 0.2987578348034904, Val f1: 0.8984770178794861\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0008631944656372, Val f1: 1.5561797618865967\n",
            "Val loss: 0.6743170817693075, Val f1: 1.0260107517242432\n",
            "Val loss: 0.6197592139244079, Val f1: 0.9197770357131958\n",
            "Val loss: 0.5859370231628418, Val f1: 0.8800166249275208\n",
            "Val loss: 0.5668998559316, Val f1: 0.8555160164833069\n",
            "\n",
            "starting Epoch 36\n",
            "Training...\n",
            "Train loss: 0.34102336689829826\n",
            "Train loss: 0.3299575321602099\n",
            "Train loss: 0.32569187343120576\n",
            "Train loss: 0.32223355458743536\n",
            "Train loss: 0.3227130029173124\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.30842603370547295, Val f1: 0.9438009262084961\n",
            "Val loss: 0.2985709273453915, Val f1: 0.9165242314338684\n",
            "Val loss: 0.2973599517345428, Val f1: 0.906559944152832\n",
            "Val loss: 0.2949555449521364, Val f1: 0.903713047504425\n",
            "Val loss: 0.29429444173971814, Val f1: 0.9012057185173035\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9962898194789886, Val f1: 1.5496731996536255\n",
            "Val loss: 0.6709324022134145, Val f1: 1.0242547988891602\n",
            "Val loss: 0.6175512135028839, Val f1: 0.917486310005188\n",
            "Val loss: 0.5841118182454791, Val f1: 0.8776400089263916\n",
            "Val loss: 0.5653654005792406, Val f1: 0.8522443175315857\n",
            "\n",
            "starting Epoch 37\n",
            "Training...\n",
            "Train loss: 0.3339622765779495\n",
            "Train loss: 0.32585584304549475\n",
            "Train loss: 0.3211140686273575\n",
            "Train loss: 0.3186069646878029\n",
            "Train loss: 0.31919111169519876\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3088309019804001, Val f1: 0.9476034641265869\n",
            "Val loss: 0.2998319954583139, Val f1: 0.9163632988929749\n",
            "Val loss: 0.29677290558815, Val f1: 0.9076288938522339\n",
            "Val loss: 0.2963666386568724, Val f1: 0.903139054775238\n",
            "Val loss: 0.2953098143140475, Val f1: 0.9005998373031616\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0155147910118103, Val f1: 1.5503830909729004\n",
            "Val loss: 0.6830254395802816, Val f1: 1.026455283164978\n",
            "Val loss: 0.6282954931259155, Val f1: 0.9198179244995117\n",
            "Val loss: 0.594076122556414, Val f1: 0.8809830546379089\n",
            "Val loss: 0.5746836298041873, Val f1: 0.8554908633232117\n",
            "\n",
            "starting Epoch 38\n",
            "Training...\n",
            "Train loss: 0.32758738100528717\n",
            "Train loss: 0.3196639897245349\n",
            "Train loss: 0.31929477393627165\n",
            "Train loss: 0.31670199935115984\n",
            "Train loss: 0.3164755506884484\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3020337428897619, Val f1: 0.9465031027793884\n",
            "Val loss: 0.290962868567669, Val f1: 0.9196639657020569\n",
            "Val loss: 0.2870865350961685, Val f1: 0.9121716022491455\n",
            "Val loss: 0.28472325072359683, Val f1: 0.9075820446014404\n",
            "Val loss: 0.28430773601645515, Val f1: 0.905035674571991\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0011094212532043, Val f1: 1.5530613660812378\n",
            "Val loss: 0.6738943854967753, Val f1: 1.0237008333206177\n",
            "Val loss: 0.6214411854743958, Val f1: 0.915643036365509\n",
            "Val loss: 0.588289030960628, Val f1: 0.876301646232605\n",
            "Val loss: 0.5699836942884657, Val f1: 0.8503106236457825\n",
            "\n",
            "starting Epoch 39\n",
            "Training...\n",
            "Train loss: 0.3244555965065956\n",
            "Train loss: 0.31908077994982403\n",
            "Train loss: 0.315803239941597\n",
            "Train loss: 0.3149005752890857\n",
            "Train loss: 0.3129252642393112\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.30137973465025425, Val f1: 0.9475564360618591\n",
            "Val loss: 0.2919661176927162, Val f1: 0.9202006459236145\n",
            "Val loss: 0.2900336620211601, Val f1: 0.9109806418418884\n",
            "Val loss: 0.28768315978014647, Val f1: 0.9062115550041199\n",
            "Val loss: 0.2869552036836034, Val f1: 0.9031664729118347\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0374958515167236, Val f1: 1.5526072978973389\n",
            "Val loss: 0.697645366191864, Val f1: 1.0269863605499268\n",
            "Val loss: 0.642439091205597, Val f1: 0.920257031917572\n",
            "Val loss: 0.6074431708880833, Val f1: 0.8813464641571045\n",
            "Val loss: 0.5878430042001936, Val f1: 0.8557130098342896\n",
            "\n",
            "starting Epoch 40\n",
            "Training...\n",
            "Train loss: 0.33160613290965557\n",
            "Train loss: 0.3181882229718295\n",
            "Train loss: 0.313769029378891\n",
            "Train loss: 0.3126749374083619\n",
            "Train loss: 0.3115977481717155\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2882922329008579, Val f1: 0.9549159407615662\n",
            "Val loss: 0.28478261376872205, Val f1: 0.9228853583335876\n",
            "Val loss: 0.28264214992523196, Val f1: 0.9129331111907959\n",
            "Val loss: 0.2822394242037588, Val f1: 0.9080054759979248\n",
            "Val loss: 0.28073995613626074, Val f1: 0.9056446552276611\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0432995557785034, Val f1: 1.5521973371505737\n",
            "Val loss: 0.7013548016548157, Val f1: 1.0243829488754272\n",
            "Val loss: 0.6468966841697693, Val f1: 0.916434109210968\n",
            "Val loss: 0.6118865438870021, Val f1: 0.8783790469169617\n",
            "Val loss: 0.5924791693687439, Val f1: 0.8528971672058105\n",
            "\n",
            "starting Epoch 41\n",
            "Training...\n",
            "Train loss: 0.3216729983687401\n",
            "Train loss: 0.308032054792751\n",
            "Train loss: 0.3080393826961517\n",
            "Train loss: 0.305448424460283\n",
            "Train loss: 0.3054456466010639\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2859353832900524, Val f1: 0.9539767503738403\n",
            "Val loss: 0.28083771918759204, Val f1: 0.9237795472145081\n",
            "Val loss: 0.27879024386405943, Val f1: 0.9146198034286499\n",
            "Val loss: 0.27714332270978104, Val f1: 0.9102795124053955\n",
            "Val loss: 0.27688285140764146, Val f1: 0.907747745513916\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0401939749717712, Val f1: 1.5521996021270752\n",
            "Val loss: 0.6993164420127869, Val f1: 1.0235365629196167\n",
            "Val loss: 0.6455624103546143, Val f1: 0.9152125716209412\n",
            "Val loss: 0.610737144947052, Val f1: 0.8764493465423584\n",
            "Val loss: 0.5915851261880662, Val f1: 0.8509199619293213\n",
            "\n",
            "starting Epoch 42\n",
            "Training...\n",
            "Train loss: 0.3200938552618027\n",
            "Train loss: 0.30506023493680084\n",
            "Train loss: 0.304417387843132\n",
            "Train loss: 0.30452236355240664\n",
            "Train loss: 0.30370362422295977\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.29440623708069324, Val f1: 0.9516047835350037\n",
            "Val loss: 0.2828681053537311, Val f1: 0.9238750338554382\n",
            "Val loss: 0.2792580834031105, Val f1: 0.9153251647949219\n",
            "Val loss: 0.27721465634765907, Val f1: 0.910621702671051\n",
            "Val loss: 0.2764443331176326, Val f1: 0.9076334834098816\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0521371364593506, Val f1: 1.553431510925293\n",
            "Val loss: 0.7065664927164713, Val f1: 1.0246050357818604\n",
            "Val loss: 0.6521412849426269, Val f1: 0.9168864488601685\n",
            "Val loss: 0.6170758179255894, Val f1: 0.8786227703094482\n",
            "Val loss: 0.5974762572182549, Val f1: 0.852509081363678\n",
            "\n",
            "starting Epoch 43\n",
            "Training...\n",
            "Train loss: 0.30910330824553967\n",
            "Train loss: 0.3055712752269976\n",
            "Train loss: 0.3014809840917587\n",
            "Train loss: 0.30168785933238357\n",
            "Train loss: 0.3005964961789903\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28916160576045513, Val f1: 0.954943060874939\n",
            "Val loss: 0.2805964296514338, Val f1: 0.9259856939315796\n",
            "Val loss: 0.275917746424675, Val f1: 0.9170100092887878\n",
            "Val loss: 0.27443478779116676, Val f1: 0.912344217300415\n",
            "Val loss: 0.2730834393629006, Val f1: 0.9106888771057129\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0475950837135315, Val f1: 1.5483791828155518\n",
            "Val loss: 0.7037416100502014, Val f1: 1.024416446685791\n",
            "Val loss: 0.6503188014030457, Val f1: 0.916111409664154\n",
            "Val loss: 0.6159865260124207, Val f1: 0.8770954608917236\n",
            "Val loss: 0.5964770913124084, Val f1: 0.851202666759491\n",
            "\n",
            "starting Epoch 44\n",
            "Training...\n",
            "Train loss: 0.3122691176831722\n",
            "Train loss: 0.3065474232037862\n",
            "Train loss: 0.30393368303775786\n",
            "Train loss: 0.3011683394659811\n",
            "Train loss: 0.2992833998231661\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28804046753793955, Val f1: 0.956278383731842\n",
            "Val loss: 0.2758816462574583, Val f1: 0.9290298223495483\n",
            "Val loss: 0.27278568863868713, Val f1: 0.9187180995941162\n",
            "Val loss: 0.27010848873586796, Val f1: 0.9152420163154602\n",
            "Val loss: 0.2707468466389747, Val f1: 0.9114559888839722\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0539867877960205, Val f1: 1.553210735321045\n",
            "Val loss: 0.707792341709137, Val f1: 1.0252948999404907\n",
            "Val loss: 0.6544057607650757, Val f1: 0.9175897836685181\n",
            "Val loss: 0.620042758328574, Val f1: 0.8777881860733032\n",
            "Val loss: 0.6002757747968038, Val f1: 0.8519389033317566\n",
            "\n",
            "starting Epoch 45\n",
            "Training...\n",
            "Train loss: 0.3103377930819988\n",
            "Train loss: 0.299420082207882\n",
            "Train loss: 0.2974294227361679\n",
            "Train loss: 0.29550655831151934\n",
            "Train loss: 0.29522099204006647\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27731923293322325, Val f1: 0.959339439868927\n",
            "Val loss: 0.26693116579995013, Val f1: 0.9317610263824463\n",
            "Val loss: 0.26421931982040403, Val f1: 0.9223230481147766\n",
            "Val loss: 0.2638184201361528, Val f1: 0.917618989944458\n",
            "Val loss: 0.262994473533971, Val f1: 0.9145960211753845\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0630210638046265, Val f1: 1.5424842834472656\n",
            "Val loss: 0.7139522631963094, Val f1: 1.0220279693603516\n",
            "Val loss: 0.6613987207412719, Val f1: 0.9142792820930481\n",
            "Val loss: 0.6270799466541835, Val f1: 0.8743352890014648\n",
            "Val loss: 0.6074328555001153, Val f1: 0.8480256795883179\n",
            "\n",
            "starting Epoch 46\n",
            "Training...\n",
            "Train loss: 0.31324267759919167\n",
            "Train loss: 0.29923190853812476\n",
            "Train loss: 0.2967249536514282\n",
            "Train loss: 0.29398559545403097\n",
            "Train loss: 0.2919815093988464\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.276263783685863, Val f1: 0.9587750434875488\n",
            "Val loss: 0.26756814483440283, Val f1: 0.930786669254303\n",
            "Val loss: 0.2657852679491043, Val f1: 0.9205268621444702\n",
            "Val loss: 0.2653496945971873, Val f1: 0.9159227013587952\n",
            "Val loss: 0.26486287053142277, Val f1: 0.9131498336791992\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.082427978515625, Val f1: 1.552727460861206\n",
            "Val loss: 0.7265342076619467, Val f1: 1.0260298252105713\n",
            "Val loss: 0.6721106767654419, Val f1: 0.9188668131828308\n",
            "Val loss: 0.6365463478224618, Val f1: 0.8783869743347168\n",
            "Val loss: 0.616261343161265, Val f1: 0.8524733781814575\n",
            "\n",
            "starting Epoch 47\n",
            "Training...\n",
            "Train loss: 0.2967284116894007\n",
            "Train loss: 0.2921991131522439\n",
            "Train loss: 0.2910970002412796\n",
            "Train loss: 0.291710095174277\n",
            "Train loss: 0.29084872773715426\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28100164234638214, Val f1: 0.9587557911872864\n",
            "Val loss: 0.26943409036506305, Val f1: 0.9304651618003845\n",
            "Val loss: 0.26560499221086503, Val f1: 0.9214974045753479\n",
            "Val loss: 0.2636296061882332, Val f1: 0.9171649217605591\n",
            "Val loss: 0.2621872265424047, Val f1: 0.9143672585487366\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.09885972738266, Val f1: 1.553269624710083\n",
            "Val loss: 0.7366337180137634, Val f1: 1.0269622802734375\n",
            "Val loss: 0.6823422908782959, Val f1: 0.9184629321098328\n",
            "Val loss: 0.646271995135716, Val f1: 0.8782759308815002\n",
            "Val loss: 0.6257838606834412, Val f1: 0.8526108264923096\n",
            "\n",
            "starting Epoch 48\n",
            "Training...\n",
            "Train loss: 0.29992310144007206\n",
            "Train loss: 0.2892571200023998\n",
            "Train loss: 0.28728846430778504\n",
            "Train loss: 0.2860649879743804\n",
            "Train loss: 0.28564160299443064\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2790579628199339, Val f1: 0.9576451182365417\n",
            "Val loss: 0.2706053758209402, Val f1: 0.929218053817749\n",
            "Val loss: 0.26785059124231336, Val f1: 0.9202954173088074\n",
            "Val loss: 0.26490671487886513, Val f1: 0.9168829321861267\n",
            "Val loss: 0.26395598905427115, Val f1: 0.9143215417861938\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0919716954231262, Val f1: 1.5530390739440918\n",
            "Val loss: 0.7334581812222799, Val f1: 1.0275096893310547\n",
            "Val loss: 0.6792021274566651, Val f1: 0.91961669921875\n",
            "Val loss: 0.6430866037096296, Val f1: 0.8808014392852783\n",
            "Val loss: 0.622302340136634, Val f1: 0.8540871143341064\n",
            "\n",
            "starting Epoch 49\n",
            "Training...\n",
            "Train loss: 0.2926838295534253\n",
            "Train loss: 0.28308964001409936\n",
            "Train loss: 0.2833117279410362\n",
            "Train loss: 0.28340247340166747\n",
            "Train loss: 0.2836441083678177\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.26945781148970127, Val f1: 0.9648405313491821\n",
            "Val loss: 0.2612300447442315, Val f1: 0.9346684813499451\n",
            "Val loss: 0.2561573141813278, Val f1: 0.9259350299835205\n",
            "Val loss: 0.25514329502831645, Val f1: 0.9208149313926697\n",
            "Val loss: 0.25432130534734043, Val f1: 0.9178608655929565\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1053817868232727, Val f1: 1.549360752105713\n",
            "Val loss: 0.7421294450759888, Val f1: 1.0229746103286743\n",
            "Val loss: 0.6887853145599365, Val f1: 0.9152582287788391\n",
            "Val loss: 0.6526364684104919, Val f1: 0.874617874622345\n",
            "Val loss: 0.6321769886546664, Val f1: 0.8485077619552612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "AdKYr4sTsdXS",
        "outputId": "fd1f30e6-a1dc-4cfd-d379-bd5864430512"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "AB8Xgh3VtCEm",
        "outputId": "053ba1ad-15ca-4b35-acb9-8bcdf86a9bc9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwcGw580tRyQ"
      },
      "source": [
        "Наша модель определенно чему-то обучилась! Ф1 скор остановился на уровне 0.75, что выше чем для предыдущей модели, функция потерь в какой-то момент была ниже 0.50 на валидационной выборке..\n",
        "\n",
        "Вывод - эта модель лучше предыдущей. Улучшений для этой модели придумывать не буду, но если улучшение для предыдущей было недостаточно убедительно, то сама вторая модель является улучшением для первой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uID14USDvAAJ"
      },
      "source": [
        "Попробуем посмотреть, что наша модель предсказывает хорошо, а что не очень:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z0M9WCvu4bj"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (text_words, text_symbs, ys) in enumerate(iterator):   \n",
        "            preds = model(text_words, text_symbs)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, text_words):\n",
        "              text = ' '.join([id2word[int(id)] for id in text if id !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOl0Lop3vkJ9"
      },
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1bvLzwnwwiG",
        "outputId": "260308a3-03bd-408c-d58b-99112b1fa22d"
      },
      "source": [
        "print('что правильно предсказываем:', tp[:10])"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "что правильно предсказываем: ['seeeptember везет а у меня вся музычка вк с а ты чего себя так на диету d', 'пользуюсь версией этого замечательного плеера от 08.11.09 приятное дополнение в windows7 наведя на значок плеера в та', 'у всех завтра 4 урока а у меня с физры ухожу d', 'для того что бы в на ногах держаться', 'доброе утро еще разок еще разок еще разок еще разок еще разок еще разок еще разок еще разок', 'началом гм все будет хорошо если трава не х', 'rt я люблю финский рок моду книги искусство не самый типичный твиттерский да d', 'ахахахахах бедные соседи ты там поаккуратней со своими', 'rt традиция как никак', '2 место в и в детском станете это же круто мы все как и уснула сразу же сегодня']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJmVX7hLxbSq"
      },
      "source": [
        "Окей, вроде действительно позитивные твиты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7iDIOM3wzGM",
        "outputId": "2fc6dbea-6119-419c-bfa4-a8afae3b9818"
      },
      "source": [
        "print('ошибочно не относим к позитивным твитам:', fn[:10])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно не относим к позитивным твитам: ['буду тихо сидеть в углу и', 'ни на секунду не пожалела что поехала с тобой', 'да не не дома же я буду в нём представил сижу в лыжах за компом бггг', 'нет бы стихи поучить но катя весь день молодежку смотрит', 'сейчас на следующей поняла,что остановку проехала', 'фест в марте так что до него полно времени', 'это хард таки даже не знаю какой смайлик поставить зрелище', 'мало кто знает что 3 бабушек когда лечат ячмень плюют ему в глаз чтобы тупо поржать', 'стоит над чем ведь и вправду так', 'уже понимаю что сильно тебя люблю и больше жить без тебя ни минуты не могу мучительные минуты до того как привезут мой гардероб']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SD2Z0UMxfIn"
      },
      "source": [
        "А вот тут уже какая-то дичь, может быть просто совпадение, но выглядит очень странно, действительно многие из этих твитов, которые размечены оригинально, как позитивные, и которые наша модель отнесла к негативным, не особо-то и позитивны.\n",
        "\n",
        "Только последний твит супер позитивный и еще один, про \"ни на секунду не пожалела\", остальные сомнительные\n",
        "\n",
        "Кстати, заметим, что во многих встречается отрицание - нет, не, ни и т.д."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXIsr5rXxKot",
        "outputId": "1e6d794a-6ccb-4fd9-de30-8f71059aa7eb"
      },
      "source": [
        "print('ошибочно считаем позитивными твитами:', fp[:10])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем позитивными твитами: ['не могу заставить мужа сходить в магазин за шоколадкой', 'суп с помидорами и ингредиенты помидоров — 1 2 килограмм', 'rt і я за вами', 'сказала подождать чуть-чуть кажется она уже забыла зачем я пришла', 'рядом нет,но в мыслях постоянно', 'чё за все номера в телефонной книге', 'осталось', 'не сигодня с вами голова болела люблю вас lt;3 мне на утро читателей 3 всем удачи завтра 3', '', 'бизнес по-русски стать на рекламу и отказывать в нужной техники']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "554RevGNyHAw"
      },
      "source": [
        "По этим твитам тоже есть вопросы, их поменьше, чем к предыдущим.\n",
        "Второй, третий, пятый, восьмой - вполне позитивные..\n",
        "Хзз! Странная конечно разметка\n",
        "Здесь никаких лингвистических штук не вижу, да и как их можно глазом увидеть, мы же анализируем работу нейросети, которая вообще на что-то там свое, только ей понятное смотрит"
      ]
    }
  ]
}